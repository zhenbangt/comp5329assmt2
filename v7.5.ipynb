{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYTGkadIZh4_"
   },
   "source": [
    "# COMP5329 - Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1621175101439,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "_HHNTgKRLXK2",
    "outputId": "3c6183f6-fc17-4847-f133-296f04a53d47"
   },
   "outputs": [],
   "source": [
    "# import google\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import torchtext\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# MOUNT_PATH = '/content/drive'\n",
    "# DRIVE_PATH = f'{MOUNT_PATH}/My Drive'\n",
    "# PROJECT_PATH = DRIVE_PATH + \"/Assignment 2\"\n",
    "PROJECT_PATH = \"./\"\n",
    "IMG_PATH = f\"{PROJECT_PATH}/data\"\n",
    "TRAIN_CSV_PATH = f\"{PROJECT_PATH}/train.csv\"\n",
    "TEST_CSV_PATH = f\"{PROJECT_PATH}/test.csv\"\n",
    "\n",
    "# google.colab.drive.mount(MOUNT_PATH)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# model = EfficientNet.from_pretrained(\"efficientnet-b0\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c78c3tjbE6d"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USEhrYiYHLSx"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1621175027575,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "lutFP6pmayNN"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pandas.read_csv(TRAIN_CSV_PATH, names=range(4), skiprows=1)\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data[3] = self.df_data[3].fillna(\"\")\n",
    "        self.df_data[2] += self.df_data[3]\n",
    "        self.df_data = self.df_data.drop(3, axis=1)\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 1: \"label\", 2: \"caption\"}, axis=1).dropna()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        label = self.df_data.iloc[idx, 1]\n",
    "        caption = self.df_data.iloc[idx, 2]\n",
    "\n",
    "        sample = {\"caption\": caption, \"label\": label, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPE_x_qeGden"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1621175027576,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "oMHDsQa5pbvJ"
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pandas.read_csv(TEST_CSV_PATH, names=range(3), skiprows=1)\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data[2] = self.df_data[2].fillna(\"\")\n",
    "        self.df_data[1] += self.df_data[2]\n",
    "        self.df_data = self.df_data.drop(2, axis=1)\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 1: \"caption\"}, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        caption = self.df_data.iloc[idx, 1]\n",
    "\n",
    "        sample = {\"caption\": caption, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R5B99nfXD3g"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMMUaz1_HJH9"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3scsujfGr18"
   },
   "source": [
    "### Dataset pre-transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1621175030901,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "zbL0EmlyeNTm"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(dataset):\n",
    "    if \"one_hot_encoded_labels\" not in dataset.tags:\n",
    "        dataset.df_data[\"label\"] = dataset.df_data[\"label\"].apply(\n",
    "            lambda l: torch.nn.functional.one_hot(\n",
    "                torch.tensor([int(i) - 1 if int(i) < 12 else int(i) - 2 for i in l.split(\" \")]), 18\n",
    "            )\n",
    "            .sum(axis=0)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        dataset.tags.add(\"one_hot_encoded_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpTCDn6_Vrco"
   },
   "source": [
    "## Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCCA7khCHOnG"
   },
   "source": [
    "### Caption embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ecrmj0tdk78V",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "lang_model = AutoModel.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "\n",
    "for name, param in lang_model.named_parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vial1naJBNC7"
   },
   "source": [
    "### Pretrained model surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "6hUrTmdVBj0Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_stem.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "blocks.0.0.conv.weight\n",
      "blocks.0.0.bn1.weight\n",
      "blocks.0.0.bn1.bias\n",
      "blocks.0.1.conv.weight\n",
      "blocks.0.1.bn1.weight\n",
      "blocks.0.1.bn1.bias\n",
      "blocks.1.0.conv_exp.weight\n",
      "blocks.1.0.bn1.weight\n",
      "blocks.1.0.bn1.bias\n",
      "blocks.1.0.conv_pwl.weight\n",
      "blocks.1.0.bn2.weight\n",
      "blocks.1.0.bn2.bias\n",
      "blocks.1.1.conv_exp.weight\n",
      "blocks.1.1.bn1.weight\n",
      "blocks.1.1.bn1.bias\n",
      "blocks.1.1.conv_pwl.weight\n",
      "blocks.1.1.bn2.weight\n",
      "blocks.1.1.bn2.bias\n",
      "blocks.1.2.conv_exp.weight\n",
      "blocks.1.2.bn1.weight\n",
      "blocks.1.2.bn1.bias\n",
      "blocks.1.2.conv_pwl.weight\n",
      "blocks.1.2.bn2.weight\n",
      "blocks.1.2.bn2.bias\n",
      "blocks.1.3.conv_exp.weight\n",
      "blocks.1.3.bn1.weight\n",
      "blocks.1.3.bn1.bias\n",
      "blocks.1.3.conv_pwl.weight\n",
      "blocks.1.3.bn2.weight\n",
      "blocks.1.3.bn2.bias\n",
      "blocks.2.0.conv_exp.weight\n",
      "blocks.2.0.bn1.weight\n",
      "blocks.2.0.bn1.bias\n",
      "blocks.2.0.conv_pwl.weight\n",
      "blocks.2.0.bn2.weight\n",
      "blocks.2.0.bn2.bias\n",
      "blocks.2.1.conv_exp.weight\n",
      "blocks.2.1.bn1.weight\n",
      "blocks.2.1.bn1.bias\n",
      "blocks.2.1.conv_pwl.weight\n",
      "blocks.2.1.bn2.weight\n",
      "blocks.2.1.bn2.bias\n",
      "blocks.2.2.conv_exp.weight\n",
      "blocks.2.2.bn1.weight\n",
      "blocks.2.2.bn1.bias\n",
      "blocks.2.2.conv_pwl.weight\n",
      "blocks.2.2.bn2.weight\n",
      "blocks.2.2.bn2.bias\n",
      "blocks.2.3.conv_exp.weight\n",
      "blocks.2.3.bn1.weight\n",
      "blocks.2.3.bn1.bias\n",
      "blocks.2.3.conv_pwl.weight\n",
      "blocks.2.3.bn2.weight\n",
      "blocks.2.3.bn2.bias\n",
      "blocks.3.0.conv_pw.weight\n",
      "blocks.3.0.bn1.weight\n",
      "blocks.3.0.bn1.bias\n",
      "blocks.3.0.conv_dw.weight\n",
      "blocks.3.0.bn2.weight\n",
      "blocks.3.0.bn2.bias\n",
      "blocks.3.0.se.conv_reduce.weight\n",
      "blocks.3.0.se.conv_reduce.bias\n",
      "blocks.3.0.se.conv_expand.weight\n",
      "blocks.3.0.se.conv_expand.bias\n",
      "blocks.3.0.conv_pwl.weight\n",
      "blocks.3.0.bn3.weight\n",
      "blocks.3.0.bn3.bias\n",
      "blocks.3.1.conv_pw.weight\n",
      "blocks.3.1.bn1.weight\n",
      "blocks.3.1.bn1.bias\n",
      "blocks.3.1.conv_dw.weight\n",
      "blocks.3.1.bn2.weight\n",
      "blocks.3.1.bn2.bias\n",
      "blocks.3.1.se.conv_reduce.weight\n",
      "blocks.3.1.se.conv_reduce.bias\n",
      "blocks.3.1.se.conv_expand.weight\n",
      "blocks.3.1.se.conv_expand.bias\n",
      "blocks.3.1.conv_pwl.weight\n",
      "blocks.3.1.bn3.weight\n",
      "blocks.3.1.bn3.bias\n",
      "blocks.3.2.conv_pw.weight\n",
      "blocks.3.2.bn1.weight\n",
      "blocks.3.2.bn1.bias\n",
      "blocks.3.2.conv_dw.weight\n",
      "blocks.3.2.bn2.weight\n",
      "blocks.3.2.bn2.bias\n",
      "blocks.3.2.se.conv_reduce.weight\n",
      "blocks.3.2.se.conv_reduce.bias\n",
      "blocks.3.2.se.conv_expand.weight\n",
      "blocks.3.2.se.conv_expand.bias\n",
      "blocks.3.2.conv_pwl.weight\n",
      "blocks.3.2.bn3.weight\n",
      "blocks.3.2.bn3.bias\n",
      "blocks.3.3.conv_pw.weight\n",
      "blocks.3.3.bn1.weight\n",
      "blocks.3.3.bn1.bias\n",
      "blocks.3.3.conv_dw.weight\n",
      "blocks.3.3.bn2.weight\n",
      "blocks.3.3.bn2.bias\n",
      "blocks.3.3.se.conv_reduce.weight\n",
      "blocks.3.3.se.conv_reduce.bias\n",
      "blocks.3.3.se.conv_expand.weight\n",
      "blocks.3.3.se.conv_expand.bias\n",
      "blocks.3.3.conv_pwl.weight\n",
      "blocks.3.3.bn3.weight\n",
      "blocks.3.3.bn3.bias\n",
      "blocks.3.4.conv_pw.weight\n",
      "blocks.3.4.bn1.weight\n",
      "blocks.3.4.bn1.bias\n",
      "blocks.3.4.conv_dw.weight\n",
      "blocks.3.4.bn2.weight\n",
      "blocks.3.4.bn2.bias\n",
      "blocks.3.4.se.conv_reduce.weight\n",
      "blocks.3.4.se.conv_reduce.bias\n",
      "blocks.3.4.se.conv_expand.weight\n",
      "blocks.3.4.se.conv_expand.bias\n",
      "blocks.3.4.conv_pwl.weight\n",
      "blocks.3.4.bn3.weight\n",
      "blocks.3.4.bn3.bias\n",
      "blocks.3.5.conv_pw.weight\n",
      "blocks.3.5.bn1.weight\n",
      "blocks.3.5.bn1.bias\n",
      "blocks.3.5.conv_dw.weight\n",
      "blocks.3.5.bn2.weight\n",
      "blocks.3.5.bn2.bias\n",
      "blocks.3.5.se.conv_reduce.weight\n",
      "blocks.3.5.se.conv_reduce.bias\n",
      "blocks.3.5.se.conv_expand.weight\n",
      "blocks.3.5.se.conv_expand.bias\n",
      "blocks.3.5.conv_pwl.weight\n",
      "blocks.3.5.bn3.weight\n",
      "blocks.3.5.bn3.bias\n",
      "blocks.4.0.conv_pw.weight\n",
      "blocks.4.0.bn1.weight\n",
      "blocks.4.0.bn1.bias\n",
      "blocks.4.0.conv_dw.weight\n",
      "blocks.4.0.bn2.weight\n",
      "blocks.4.0.bn2.bias\n",
      "blocks.4.0.se.conv_reduce.weight\n",
      "blocks.4.0.se.conv_reduce.bias\n",
      "blocks.4.0.se.conv_expand.weight\n",
      "blocks.4.0.se.conv_expand.bias\n",
      "blocks.4.0.conv_pwl.weight\n",
      "blocks.4.0.bn3.weight\n",
      "blocks.4.0.bn3.bias\n",
      "blocks.4.1.conv_pw.weight\n",
      "blocks.4.1.bn1.weight\n",
      "blocks.4.1.bn1.bias\n",
      "blocks.4.1.conv_dw.weight\n",
      "blocks.4.1.bn2.weight\n",
      "blocks.4.1.bn2.bias\n",
      "blocks.4.1.se.conv_reduce.weight\n",
      "blocks.4.1.se.conv_reduce.bias\n",
      "blocks.4.1.se.conv_expand.weight\n",
      "blocks.4.1.se.conv_expand.bias\n",
      "blocks.4.1.conv_pwl.weight\n",
      "blocks.4.1.bn3.weight\n",
      "blocks.4.1.bn3.bias\n",
      "blocks.4.2.conv_pw.weight\n",
      "blocks.4.2.bn1.weight\n",
      "blocks.4.2.bn1.bias\n",
      "blocks.4.2.conv_dw.weight\n",
      "blocks.4.2.bn2.weight\n",
      "blocks.4.2.bn2.bias\n",
      "blocks.4.2.se.conv_reduce.weight\n",
      "blocks.4.2.se.conv_reduce.bias\n",
      "blocks.4.2.se.conv_expand.weight\n",
      "blocks.4.2.se.conv_expand.bias\n",
      "blocks.4.2.conv_pwl.weight\n",
      "blocks.4.2.bn3.weight\n",
      "blocks.4.2.bn3.bias\n",
      "blocks.4.3.conv_pw.weight\n",
      "blocks.4.3.bn1.weight\n",
      "blocks.4.3.bn1.bias\n",
      "blocks.4.3.conv_dw.weight\n",
      "blocks.4.3.bn2.weight\n",
      "blocks.4.3.bn2.bias\n",
      "blocks.4.3.se.conv_reduce.weight\n",
      "blocks.4.3.se.conv_reduce.bias\n",
      "blocks.4.3.se.conv_expand.weight\n",
      "blocks.4.3.se.conv_expand.bias\n",
      "blocks.4.3.conv_pwl.weight\n",
      "blocks.4.3.bn3.weight\n",
      "blocks.4.3.bn3.bias\n",
      "blocks.4.4.conv_pw.weight\n",
      "blocks.4.4.bn1.weight\n",
      "blocks.4.4.bn1.bias\n",
      "blocks.4.4.conv_dw.weight\n",
      "blocks.4.4.bn2.weight\n",
      "blocks.4.4.bn2.bias\n",
      "blocks.4.4.se.conv_reduce.weight\n",
      "blocks.4.4.se.conv_reduce.bias\n",
      "blocks.4.4.se.conv_expand.weight\n",
      "blocks.4.4.se.conv_expand.bias\n",
      "blocks.4.4.conv_pwl.weight\n",
      "blocks.4.4.bn3.weight\n",
      "blocks.4.4.bn3.bias\n",
      "blocks.4.5.conv_pw.weight\n",
      "blocks.4.5.bn1.weight\n",
      "blocks.4.5.bn1.bias\n",
      "blocks.4.5.conv_dw.weight\n",
      "blocks.4.5.bn2.weight\n",
      "blocks.4.5.bn2.bias\n",
      "blocks.4.5.se.conv_reduce.weight\n",
      "blocks.4.5.se.conv_reduce.bias\n",
      "blocks.4.5.se.conv_expand.weight\n",
      "blocks.4.5.se.conv_expand.bias\n",
      "blocks.4.5.conv_pwl.weight\n",
      "blocks.4.5.bn3.weight\n",
      "blocks.4.5.bn3.bias\n",
      "blocks.4.6.conv_pw.weight\n",
      "blocks.4.6.bn1.weight\n",
      "blocks.4.6.bn1.bias\n",
      "blocks.4.6.conv_dw.weight\n",
      "blocks.4.6.bn2.weight\n",
      "blocks.4.6.bn2.bias\n",
      "blocks.4.6.se.conv_reduce.weight\n",
      "blocks.4.6.se.conv_reduce.bias\n",
      "blocks.4.6.se.conv_expand.weight\n",
      "blocks.4.6.se.conv_expand.bias\n",
      "blocks.4.6.conv_pwl.weight\n",
      "blocks.4.6.bn3.weight\n",
      "blocks.4.6.bn3.bias\n",
      "blocks.4.7.conv_pw.weight\n",
      "blocks.4.7.bn1.weight\n",
      "blocks.4.7.bn1.bias\n",
      "blocks.4.7.conv_dw.weight\n",
      "blocks.4.7.bn2.weight\n",
      "blocks.4.7.bn2.bias\n",
      "blocks.4.7.se.conv_reduce.weight\n",
      "blocks.4.7.se.conv_reduce.bias\n",
      "blocks.4.7.se.conv_expand.weight\n",
      "blocks.4.7.se.conv_expand.bias\n",
      "blocks.4.7.conv_pwl.weight\n",
      "blocks.4.7.bn3.weight\n",
      "blocks.4.7.bn3.bias\n",
      "blocks.4.8.conv_pw.weight\n",
      "blocks.4.8.bn1.weight\n",
      "blocks.4.8.bn1.bias\n",
      "blocks.4.8.conv_dw.weight\n",
      "blocks.4.8.bn2.weight\n",
      "blocks.4.8.bn2.bias\n",
      "blocks.4.8.se.conv_reduce.weight\n",
      "blocks.4.8.se.conv_reduce.bias\n",
      "blocks.4.8.se.conv_expand.weight\n",
      "blocks.4.8.se.conv_expand.bias\n",
      "blocks.4.8.conv_pwl.weight\n",
      "blocks.4.8.bn3.weight\n",
      "blocks.4.8.bn3.bias\n",
      "blocks.5.0.conv_pw.weight\n",
      "blocks.5.0.bn1.weight\n",
      "blocks.5.0.bn1.bias\n",
      "blocks.5.0.conv_dw.weight\n",
      "blocks.5.0.bn2.weight\n",
      "blocks.5.0.bn2.bias\n",
      "blocks.5.0.se.conv_reduce.weight\n",
      "blocks.5.0.se.conv_reduce.bias\n",
      "blocks.5.0.se.conv_expand.weight\n",
      "blocks.5.0.se.conv_expand.bias\n",
      "blocks.5.0.conv_pwl.weight\n",
      "blocks.5.0.bn3.weight\n",
      "blocks.5.0.bn3.bias\n",
      "blocks.5.1.conv_pw.weight\n",
      "blocks.5.1.bn1.weight\n",
      "blocks.5.1.bn1.bias\n",
      "blocks.5.1.conv_dw.weight\n",
      "blocks.5.1.bn2.weight\n",
      "blocks.5.1.bn2.bias\n",
      "blocks.5.1.se.conv_reduce.weight\n",
      "blocks.5.1.se.conv_reduce.bias\n",
      "blocks.5.1.se.conv_expand.weight\n",
      "blocks.5.1.se.conv_expand.bias\n",
      "blocks.5.1.conv_pwl.weight\n",
      "blocks.5.1.bn3.weight\n",
      "blocks.5.1.bn3.bias\n",
      "blocks.5.2.conv_pw.weight\n",
      "blocks.5.2.bn1.weight\n",
      "blocks.5.2.bn1.bias\n",
      "blocks.5.2.conv_dw.weight\n",
      "blocks.5.2.bn2.weight\n",
      "blocks.5.2.bn2.bias\n",
      "blocks.5.2.se.conv_reduce.weight\n",
      "blocks.5.2.se.conv_reduce.bias\n",
      "blocks.5.2.se.conv_expand.weight\n",
      "blocks.5.2.se.conv_expand.bias\n",
      "blocks.5.2.conv_pwl.weight\n",
      "blocks.5.2.bn3.weight\n",
      "blocks.5.2.bn3.bias\n",
      "blocks.5.3.conv_pw.weight\n",
      "blocks.5.3.bn1.weight\n",
      "blocks.5.3.bn1.bias\n",
      "blocks.5.3.conv_dw.weight\n",
      "blocks.5.3.bn2.weight\n",
      "blocks.5.3.bn2.bias\n",
      "blocks.5.3.se.conv_reduce.weight\n",
      "blocks.5.3.se.conv_reduce.bias\n",
      "blocks.5.3.se.conv_expand.weight\n",
      "blocks.5.3.se.conv_expand.bias\n",
      "blocks.5.3.conv_pwl.weight\n",
      "blocks.5.3.bn3.weight\n",
      "blocks.5.3.bn3.bias\n",
      "blocks.5.4.conv_pw.weight\n",
      "blocks.5.4.bn1.weight\n",
      "blocks.5.4.bn1.bias\n",
      "blocks.5.4.conv_dw.weight\n",
      "blocks.5.4.bn2.weight\n",
      "blocks.5.4.bn2.bias\n",
      "blocks.5.4.se.conv_reduce.weight\n",
      "blocks.5.4.se.conv_reduce.bias\n",
      "blocks.5.4.se.conv_expand.weight\n",
      "blocks.5.4.se.conv_expand.bias\n",
      "blocks.5.4.conv_pwl.weight\n",
      "blocks.5.4.bn3.weight\n",
      "blocks.5.4.bn3.bias\n",
      "blocks.5.5.conv_pw.weight\n",
      "blocks.5.5.bn1.weight\n",
      "blocks.5.5.bn1.bias\n",
      "blocks.5.5.conv_dw.weight\n",
      "blocks.5.5.bn2.weight\n",
      "blocks.5.5.bn2.bias\n",
      "blocks.5.5.se.conv_reduce.weight\n",
      "blocks.5.5.se.conv_reduce.bias\n",
      "blocks.5.5.se.conv_expand.weight\n",
      "blocks.5.5.se.conv_expand.bias\n",
      "blocks.5.5.conv_pwl.weight\n",
      "blocks.5.5.bn3.weight\n",
      "blocks.5.5.bn3.bias\n",
      "blocks.5.6.conv_pw.weight\n",
      "blocks.5.6.bn1.weight\n",
      "blocks.5.6.bn1.bias\n",
      "blocks.5.6.conv_dw.weight\n",
      "blocks.5.6.bn2.weight\n",
      "blocks.5.6.bn2.bias\n",
      "blocks.5.6.se.conv_reduce.weight\n",
      "blocks.5.6.se.conv_reduce.bias\n",
      "blocks.5.6.se.conv_expand.weight\n",
      "blocks.5.6.se.conv_expand.bias\n",
      "blocks.5.6.conv_pwl.weight\n",
      "blocks.5.6.bn3.weight\n",
      "blocks.5.6.bn3.bias\n",
      "blocks.5.7.conv_pw.weight\n",
      "blocks.5.7.bn1.weight\n",
      "blocks.5.7.bn1.bias\n",
      "blocks.5.7.conv_dw.weight\n",
      "blocks.5.7.bn2.weight\n",
      "blocks.5.7.bn2.bias\n",
      "blocks.5.7.se.conv_reduce.weight\n",
      "blocks.5.7.se.conv_reduce.bias\n",
      "blocks.5.7.se.conv_expand.weight\n",
      "blocks.5.7.se.conv_expand.bias\n",
      "blocks.5.7.conv_pwl.weight\n",
      "blocks.5.7.bn3.weight\n",
      "blocks.5.7.bn3.bias\n",
      "blocks.5.8.conv_pw.weight\n",
      "blocks.5.8.bn1.weight\n",
      "blocks.5.8.bn1.bias\n",
      "blocks.5.8.conv_dw.weight\n",
      "blocks.5.8.bn2.weight\n",
      "blocks.5.8.bn2.bias\n",
      "blocks.5.8.se.conv_reduce.weight\n",
      "blocks.5.8.se.conv_reduce.bias\n",
      "blocks.5.8.se.conv_expand.weight\n",
      "blocks.5.8.se.conv_expand.bias\n",
      "blocks.5.8.conv_pwl.weight\n",
      "blocks.5.8.bn3.weight\n",
      "blocks.5.8.bn3.bias\n",
      "blocks.5.9.conv_pw.weight\n",
      "blocks.5.9.bn1.weight\n",
      "blocks.5.9.bn1.bias\n",
      "blocks.5.9.conv_dw.weight\n",
      "blocks.5.9.bn2.weight\n",
      "blocks.5.9.bn2.bias\n",
      "blocks.5.9.se.conv_reduce.weight\n",
      "blocks.5.9.se.conv_reduce.bias\n",
      "blocks.5.9.se.conv_expand.weight\n",
      "blocks.5.9.se.conv_expand.bias\n",
      "blocks.5.9.conv_pwl.weight\n",
      "blocks.5.9.bn3.weight\n",
      "blocks.5.9.bn3.bias\n",
      "blocks.5.10.conv_pw.weight\n",
      "blocks.5.10.bn1.weight\n",
      "blocks.5.10.bn1.bias\n",
      "blocks.5.10.conv_dw.weight\n",
      "blocks.5.10.bn2.weight\n",
      "blocks.5.10.bn2.bias\n",
      "blocks.5.10.se.conv_reduce.weight\n",
      "blocks.5.10.se.conv_reduce.bias\n",
      "blocks.5.10.se.conv_expand.weight\n",
      "blocks.5.10.se.conv_expand.bias\n",
      "blocks.5.10.conv_pwl.weight\n",
      "blocks.5.10.bn3.weight\n",
      "blocks.5.10.bn3.bias\n",
      "blocks.5.11.conv_pw.weight\n",
      "blocks.5.11.bn1.weight\n",
      "blocks.5.11.bn1.bias\n",
      "blocks.5.11.conv_dw.weight\n",
      "blocks.5.11.bn2.weight\n",
      "blocks.5.11.bn2.bias\n",
      "blocks.5.11.se.conv_reduce.weight\n",
      "blocks.5.11.se.conv_reduce.bias\n",
      "blocks.5.11.se.conv_expand.weight\n",
      "blocks.5.11.se.conv_expand.bias\n",
      "blocks.5.11.conv_pwl.weight\n",
      "blocks.5.11.bn3.weight\n",
      "blocks.5.11.bn3.bias\n",
      "blocks.5.12.conv_pw.weight\n",
      "blocks.5.12.bn1.weight\n",
      "blocks.5.12.bn1.bias\n",
      "blocks.5.12.conv_dw.weight\n",
      "blocks.5.12.bn2.weight\n",
      "blocks.5.12.bn2.bias\n",
      "blocks.5.12.se.conv_reduce.weight\n",
      "blocks.5.12.se.conv_reduce.bias\n",
      "blocks.5.12.se.conv_expand.weight\n",
      "blocks.5.12.se.conv_expand.bias\n",
      "blocks.5.12.conv_pwl.weight\n",
      "blocks.5.12.bn3.weight\n",
      "blocks.5.12.bn3.bias\n",
      "blocks.5.13.conv_pw.weight\n",
      "blocks.5.13.bn1.weight\n",
      "blocks.5.13.bn1.bias\n",
      "blocks.5.13.conv_dw.weight\n",
      "blocks.5.13.bn2.weight\n",
      "blocks.5.13.bn2.bias\n",
      "blocks.5.13.se.conv_reduce.weight\n",
      "blocks.5.13.se.conv_reduce.bias\n",
      "blocks.5.13.se.conv_expand.weight\n",
      "blocks.5.13.se.conv_expand.bias\n",
      "blocks.5.13.conv_pwl.weight\n",
      "blocks.5.13.bn3.weight\n",
      "blocks.5.13.bn3.bias\n",
      "blocks.5.14.conv_pw.weight\n",
      "blocks.5.14.bn1.weight\n",
      "blocks.5.14.bn1.bias\n",
      "blocks.5.14.conv_dw.weight\n",
      "blocks.5.14.bn2.weight\n",
      "blocks.5.14.bn2.bias\n",
      "blocks.5.14.se.conv_reduce.weight\n",
      "blocks.5.14.se.conv_reduce.bias\n",
      "blocks.5.14.se.conv_expand.weight\n",
      "blocks.5.14.se.conv_expand.bias\n",
      "blocks.5.14.conv_pwl.weight\n",
      "blocks.5.14.bn3.weight\n",
      "blocks.5.14.bn3.bias\n",
      "conv_head.weight\n",
      "bn2.weight\n",
      "bn2.bias\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19}\n",
    "features = timm.create_model(\"tf_efficientnetv2_s_in21k\", pretrained=True, num_classes=0)\n",
    "\n",
    "for name, param in features.named_parameters():\n",
    "    param.requires_grad_(False)\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "EfficientNet                                  --                        --\n",
       "├─Conv2dSame: 1-1                             [1, 24, 112, 112]         (648)\n",
       "├─BatchNorm2d: 1-2                            [1, 24, 112, 112]         (48)\n",
       "├─SiLU: 1-3                                   [1, 24, 112, 112]         --\n",
       "├─Sequential: 1-4                             [1, 256, 7, 7]            --\n",
       "│    └─Sequential: 2-1                        [1, 24, 112, 112]         --\n",
       "│    │    └─ConvBnAct: 3-1                    [1, 24, 112, 112]         (5,232)\n",
       "│    │    └─ConvBnAct: 3-2                    [1, 24, 112, 112]         (5,232)\n",
       "│    └─Sequential: 2-2                        [1, 48, 56, 56]           --\n",
       "│    │    └─EdgeResidual: 3-3                 [1, 48, 56, 56]           (25,632)\n",
       "│    │    └─EdgeResidual: 3-4                 [1, 48, 56, 56]           (92,640)\n",
       "│    │    └─EdgeResidual: 3-5                 [1, 48, 56, 56]           (92,640)\n",
       "│    │    └─EdgeResidual: 3-6                 [1, 48, 56, 56]           (92,640)\n",
       "│    └─Sequential: 2-3                        [1, 64, 28, 28]           --\n",
       "│    │    └─EdgeResidual: 3-7                 [1, 64, 28, 28]           (95,744)\n",
       "│    │    └─EdgeResidual: 3-8                 [1, 64, 28, 28]           (164,480)\n",
       "│    │    └─EdgeResidual: 3-9                 [1, 64, 28, 28]           (164,480)\n",
       "│    │    └─EdgeResidual: 3-10                [1, 64, 28, 28]           (164,480)\n",
       "│    └─Sequential: 2-4                        [1, 128, 14, 14]          --\n",
       "│    │    └─InvertedResidual: 3-11            [1, 128, 14, 14]          (61,200)\n",
       "│    │    └─InvertedResidual: 3-12            [1, 128, 14, 14]          (171,296)\n",
       "│    │    └─InvertedResidual: 3-13            [1, 128, 14, 14]          (171,296)\n",
       "│    │    └─InvertedResidual: 3-14            [1, 128, 14, 14]          (171,296)\n",
       "│    │    └─InvertedResidual: 3-15            [1, 128, 14, 14]          (171,296)\n",
       "│    │    └─InvertedResidual: 3-16            [1, 128, 14, 14]          (171,296)\n",
       "│    └─Sequential: 2-5                        [1, 160, 14, 14]          --\n",
       "│    │    └─InvertedResidual: 3-17            [1, 160, 14, 14]          (281,440)\n",
       "│    │    └─InvertedResidual: 3-18            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-19            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-20            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-21            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-22            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-23            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-24            [1, 160, 14, 14]          (397,800)\n",
       "│    │    └─InvertedResidual: 3-25            [1, 160, 14, 14]          (397,800)\n",
       "│    └─Sequential: 2-6                        [1, 256, 7, 7]            --\n",
       "│    │    └─InvertedResidual: 3-26            [1, 256, 7, 7]            (490,152)\n",
       "│    │    └─InvertedResidual: 3-27            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-28            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-29            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-30            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-31            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-32            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-33            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-34            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-35            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-36            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-37            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-38            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-39            [1, 256, 7, 7]            (1,005,120)\n",
       "│    │    └─InvertedResidual: 3-40            [1, 256, 7, 7]            (1,005,120)\n",
       "├─Conv2d: 1-5                                 [1, 1280, 7, 7]           (327,680)\n",
       "├─BatchNorm2d: 1-6                            [1, 1280, 7, 7]           (2,560)\n",
       "├─SiLU: 1-7                                   [1, 1280, 7, 7]           --\n",
       "├─SelectAdaptivePool2d: 1-8                   [1, 1280]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-7                 [1, 1280, 1, 1]           --\n",
       "├─Identity: 1-9                               [1, 1280]                 --\n",
       "===============================================================================================\n",
       "Total params: 20,177,488\n",
       "Trainable params: 0\n",
       "Non-trainable params: 20,177,488\n",
       "Total mult-adds (G): 2.85\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 194.81\n",
       "Params size (MB): 80.71\n",
       "Estimated Total Size (MB): 276.13\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(features, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v44fbNqCOtv9"
   },
   "source": [
    "### Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aUtVf9QdOxnN"
   },
   "outputs": [],
   "source": [
    "NUM_ClASSES = 18\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "img_dim = 1280\n",
    "word_dim = 256\n",
    "\n",
    "\n",
    "class Combined_Model(torch.nn.Module):\n",
    "    def __init__(self, visual_features, lang_model):\n",
    "        super().__init__()\n",
    "        self.visual_features = visual_features\n",
    "        self.lang_model = lang_model\n",
    "        self.proj = nn.Linear(img_dim, word_dim)  # (feature.shape, hidden.shape)\n",
    "        self.scale = math.sqrt(word_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.combine = nn.Linear(\n",
    "            word_dim + img_dim + word_dim, NUM_ClASSES\n",
    "        )  # (feature.shape + hidden.shape + hidden.shape)\n",
    "        #         self.hidden1 = nn.Linear(256, 128)\n",
    "\n",
    "    #         self.output = nn.Linear(256, NUM_ClASSES)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):  # requires tokenized captions\n",
    "        images = self.visual_features(images)\n",
    "        proj = self.proj(images)\n",
    "\n",
    "        lang_outs = self.lang_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = lang_outs[\"last_hidden_state\"]\n",
    "        pooler_out = lang_outs[\"pooler_output\"]\n",
    "\n",
    "        attention_weights = (\n",
    "            torch.bmm(hidden, proj.unsqueeze(-1)).squeeze(-1) / self.scale\n",
    "        )  # (batch, seq_len, 1)\n",
    "        attention_out = torch.bmm(attention_weights.unsqueeze(1), hidden).squeeze(1)\n",
    "\n",
    "        out = torch.cat([images, attention_out, pooler_out], dim=-1)\n",
    "        out = self.combine(out)\n",
    "        #         out = self.activation(out)\n",
    "        #         out = self.hidden1(out)\n",
    "        #         out = self.activation(out)\n",
    "        #         out = self.output(out)\n",
    "        #         out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srZFInwa78_j"
   },
   "source": [
    "## Transforms and training support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1621175053522,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "MvOT-Plt78Zp"
   },
   "outputs": [],
   "source": [
    "class FieldTransform(object):\n",
    "    def __init__(self, field, transform):\n",
    "        self.field = field\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample[self.field] = self.transform(sample[self.field])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZHM_qQiQniU"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1621175053523,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "oPqavf7wWfiB"
   },
   "outputs": [],
   "source": [
    "def train_collate_fn(X):\n",
    "    # convert [{key: val, ...}, ...]\n",
    "    # to [key: [val, ...],  ...}\n",
    "    X = {k: [v[k] for v in X] for k in X[0]}\n",
    "    X[\"label\"] = torch.stack(X[\"label\"])\n",
    "    X[\"image\"] = torch.stack(X[\"image\"])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        #   FieldTransform('image', torchvision.transforms.Resize((64, 64))),\n",
    "        #   FieldTransform('image', torchvision.transforms.CenterCrop(64)),\n",
    "        FieldTransform(\"image\", torchvision.transforms.Resize((224, 224))),\n",
    "        FieldTransform(\"image\", torchvision.transforms.ToTensor()),\n",
    "        FieldTransform(\"image\", normalize),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2421,
     "status": "ok",
     "timestamp": 1621175110701,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "o_5acvhCWjSn"
   },
   "outputs": [],
   "source": [
    "train_data = TrainDataset(transform=transforms)\n",
    "# vocabularise_caption(train_data, vocab)\n",
    "one_hot_encode_labels(train_data)\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# train_data.__len__() == 30000\n",
    "trainds, valds = random_split(train_data, [27000, 3000])\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    trainds, batch_size=32, shuffle=True, collate_fn=train_collate_fn, num_workers=24\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valds, batch_size=8, shuffle=False, collate_fn=train_collate_fn, num_workers=24)\n",
    "\n",
    "# model = Combined().to(DEVICE)\n",
    "model = Combined_Model(features, lang_model).to(device)  # call fp16 for model after training!\n",
    "# optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from sam import SAM\n",
    "\n",
    "base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=0.005)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# torch.save(model, \"test_v7.5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuhLuCQxQnR1",
    "outputId": "bde8bc1d-7820-460e-8d89-e78e8af216d4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_features.blocks.5.7.conv_pw.weight\n",
      "visual_features.blocks.5.7.bn1.weight\n",
      "visual_features.blocks.5.7.bn1.bias\n",
      "visual_features.blocks.5.7.conv_dw.weight\n",
      "visual_features.blocks.5.7.bn2.weight\n",
      "visual_features.blocks.5.7.bn2.bias\n",
      "visual_features.blocks.5.7.se.conv_reduce.weight\n",
      "visual_features.blocks.5.7.se.conv_reduce.bias\n",
      "visual_features.blocks.5.7.se.conv_expand.weight\n",
      "visual_features.blocks.5.7.se.conv_expand.bias\n",
      "visual_features.blocks.5.7.conv_pwl.weight\n",
      "visual_features.blocks.5.7.bn3.weight\n",
      "visual_features.blocks.5.7.bn3.bias\n",
      "visual_features.blocks.5.8.conv_pw.weight\n",
      "visual_features.blocks.5.8.bn1.weight\n",
      "visual_features.blocks.5.8.bn1.bias\n",
      "visual_features.blocks.5.8.conv_dw.weight\n",
      "visual_features.blocks.5.8.bn2.weight\n",
      "visual_features.blocks.5.8.bn2.bias\n",
      "visual_features.blocks.5.8.se.conv_reduce.weight\n",
      "visual_features.blocks.5.8.se.conv_reduce.bias\n",
      "visual_features.blocks.5.8.se.conv_expand.weight\n",
      "visual_features.blocks.5.8.se.conv_expand.bias\n",
      "visual_features.blocks.5.8.conv_pwl.weight\n",
      "visual_features.blocks.5.8.bn3.weight\n",
      "visual_features.blocks.5.8.bn3.bias\n",
      "visual_features.blocks.5.9.conv_pw.weight\n",
      "visual_features.blocks.5.9.bn1.weight\n",
      "visual_features.blocks.5.9.bn1.bias\n",
      "visual_features.blocks.5.9.conv_dw.weight\n",
      "visual_features.blocks.5.9.bn2.weight\n",
      "visual_features.blocks.5.9.bn2.bias\n",
      "visual_features.blocks.5.9.se.conv_reduce.weight\n",
      "visual_features.blocks.5.9.se.conv_reduce.bias\n",
      "visual_features.blocks.5.9.se.conv_expand.weight\n",
      "visual_features.blocks.5.9.se.conv_expand.bias\n",
      "visual_features.blocks.5.9.conv_pwl.weight\n",
      "visual_features.blocks.5.9.bn3.weight\n",
      "visual_features.blocks.5.9.bn3.bias\n",
      "visual_features.blocks.5.10.conv_pw.weight\n",
      "visual_features.blocks.5.10.bn1.weight\n",
      "visual_features.blocks.5.10.bn1.bias\n",
      "visual_features.blocks.5.10.conv_dw.weight\n",
      "visual_features.blocks.5.10.bn2.weight\n",
      "visual_features.blocks.5.10.bn2.bias\n",
      "visual_features.blocks.5.10.se.conv_reduce.weight\n",
      "visual_features.blocks.5.10.se.conv_reduce.bias\n",
      "visual_features.blocks.5.10.se.conv_expand.weight\n",
      "visual_features.blocks.5.10.se.conv_expand.bias\n",
      "visual_features.blocks.5.10.conv_pwl.weight\n",
      "visual_features.blocks.5.10.bn3.weight\n",
      "visual_features.blocks.5.10.bn3.bias\n",
      "visual_features.blocks.5.11.conv_pw.weight\n",
      "visual_features.blocks.5.11.bn1.weight\n",
      "visual_features.blocks.5.11.bn1.bias\n",
      "visual_features.blocks.5.11.conv_dw.weight\n",
      "visual_features.blocks.5.11.bn2.weight\n",
      "visual_features.blocks.5.11.bn2.bias\n",
      "visual_features.blocks.5.11.se.conv_reduce.weight\n",
      "visual_features.blocks.5.11.se.conv_reduce.bias\n",
      "visual_features.blocks.5.11.se.conv_expand.weight\n",
      "visual_features.blocks.5.11.se.conv_expand.bias\n",
      "visual_features.blocks.5.11.conv_pwl.weight\n",
      "visual_features.blocks.5.11.bn3.weight\n",
      "visual_features.blocks.5.11.bn3.bias\n",
      "visual_features.blocks.5.12.conv_pw.weight\n",
      "visual_features.blocks.5.12.bn1.weight\n",
      "visual_features.blocks.5.12.bn1.bias\n",
      "visual_features.blocks.5.12.conv_dw.weight\n",
      "visual_features.blocks.5.12.bn2.weight\n",
      "visual_features.blocks.5.12.bn2.bias\n",
      "visual_features.blocks.5.12.se.conv_reduce.weight\n",
      "visual_features.blocks.5.12.se.conv_reduce.bias\n",
      "visual_features.blocks.5.12.se.conv_expand.weight\n",
      "visual_features.blocks.5.12.se.conv_expand.bias\n",
      "visual_features.blocks.5.12.conv_pwl.weight\n",
      "visual_features.blocks.5.12.bn3.weight\n",
      "visual_features.blocks.5.12.bn3.bias\n",
      "visual_features.blocks.5.13.conv_pw.weight\n",
      "visual_features.blocks.5.13.bn1.weight\n",
      "visual_features.blocks.5.13.bn1.bias\n",
      "visual_features.blocks.5.13.conv_dw.weight\n",
      "visual_features.blocks.5.13.bn2.weight\n",
      "visual_features.blocks.5.13.bn2.bias\n",
      "visual_features.blocks.5.13.se.conv_reduce.weight\n",
      "visual_features.blocks.5.13.se.conv_reduce.bias\n",
      "visual_features.blocks.5.13.se.conv_expand.weight\n",
      "visual_features.blocks.5.13.se.conv_expand.bias\n",
      "visual_features.blocks.5.13.conv_pwl.weight\n",
      "visual_features.blocks.5.13.bn3.weight\n",
      "visual_features.blocks.5.13.bn3.bias\n",
      "visual_features.blocks.5.14.conv_pw.weight\n",
      "visual_features.blocks.5.14.bn1.weight\n",
      "visual_features.blocks.5.14.bn1.bias\n",
      "visual_features.blocks.5.14.conv_dw.weight\n",
      "visual_features.blocks.5.14.bn2.weight\n",
      "visual_features.blocks.5.14.bn2.bias\n",
      "visual_features.blocks.5.14.se.conv_reduce.weight\n",
      "visual_features.blocks.5.14.se.conv_reduce.bias\n",
      "visual_features.blocks.5.14.se.conv_expand.weight\n",
      "visual_features.blocks.5.14.se.conv_expand.bias\n",
      "visual_features.blocks.5.14.conv_pwl.weight\n",
      "visual_features.blocks.5.14.bn3.weight\n",
      "visual_features.blocks.5.14.bn3.bias\n",
      "visual_features.conv_head.weight\n",
      "visual_features.bn2.weight\n",
      "visual_features.bn2.bias\n",
      "lang_model.pooler.dense.weight\n",
      "lang_model.pooler.dense.bias\n",
      "proj.weight\n",
      "proj.bias\n",
      "combine.weight\n",
      "combine.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:48<00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, train/val loss:(0.1836194, 0.1227298),sample/mean f1:(0.7170794, 0.7150312), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:49<00:00, 17.08it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:1, train/val loss:(0.1058283, 0.0944373),sample/mean f1:(0.8155049, 0.7938279), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:49<00:00, 17.19it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:2, train/val loss:(0.0857253, 0.0852269),sample/mean f1:(0.8386316, 0.8155363), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:50<00:00, 16.77it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:3, train/val loss:(0.0732917, 0.0814409),sample/mean f1:(0.8494083, 0.826087), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:55<00:00, 15.23it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:4, train/val loss:(0.0631703, 0.0832293),sample/mean f1:(0.8533088, 0.826552), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:50<00:00, 16.59it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5, train/val loss:(0.0542496, 0.0822721),sample/mean f1:(0.8512507, 0.8276902), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:55<00:00, 15.25it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:6, train/val loss:(0.0456066, 0.085484),sample/mean f1:(0.8540116, 0.8297456), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:50<00:00, 16.73it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving best\n",
      "Epoch:7, train/val loss:(0.0372635, 0.0924296),sample/mean f1:(0.8580345, 0.8283084), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:49<00:00, 17.01it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8, train/val loss:(0.0295592, 0.0968568),sample/mean f1:(0.8525075, 0.8227834), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:50<00:00, 16.59it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9, train/val loss:(0.0230095, 0.1014413),sample/mean f1:(0.8507302, 0.8252737), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:48<00:00, 17.35it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10, train/val loss:(0.017863, 0.1080543),sample/mean f1:(0.8510242, 0.8234495), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:48<00:00, 17.39it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11, train/val loss:(0.01389, 0.1176253),sample/mean f1:(0.8488902, 0.8214408), lr:5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 631/844 [02:03<00:41,  5.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1279db2a0ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#             loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-56151a2ad275>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# requires tokenized captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "val_mean_f1 = []\n",
    "val_sample_f1 = []\n",
    "model_in_memory = {}\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(20):\n",
    "    train_loss = []\n",
    "    train_outs = []\n",
    "    train_lables = []\n",
    "    model.train()\n",
    "\n",
    "    # fine tune after 5 epoches\n",
    "    if epoch == 0:\n",
    "        finetune_param = False\n",
    "        for name, param in model.visual_features.named_parameters():\n",
    "            #     print(name.split(\".\")[1])\n",
    "            if \"blocks.5.7\" in name:  # 22; required grad for last 2 blocks\n",
    "                finetune_param = True\n",
    "            if finetune_param:\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "        finetune_param = False\n",
    "        for name, param in model.lang_model.named_parameters():\n",
    "            if \"pooler\" in name.split(\".\"):\n",
    "                finetune_param = True\n",
    "            if finetune_param:\n",
    "                param.requires_grad_(True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dl)):\n",
    "        #         optim.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        captions = batch[\"caption\"]\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "        # first second step\n",
    "       \n",
    "        \n",
    "            predictions = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "#             loss.backward()\n",
    "    #         optimizer.first_step(zero_grad=True)\n",
    "#             optimizer.step()\n",
    "\n",
    "        #         with torch.cuda.amp.autocast():\n",
    "        #             # second step\n",
    "#         predictions = model(images, input_ids, attention_mask)\n",
    "#         loss = criterion(predictions, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.second_step(zero_grad=True)\n",
    "#         #\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    val_loss = []\n",
    "    val_outs = []\n",
    "    val_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(val_dl):\n",
    "\n",
    "        captions = batch[\"caption\"]\n",
    "        images = batch[\"image\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        predictions = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        val_outs.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "    val_labels = np.vstack(val_labels)\n",
    "    val_outs = np.vstack(val_outs)\n",
    "    mean_f1 = f1_score(y_true=val_labels, y_pred=1 * (val_outs > 0), average=\"micro\")  # mean f1\n",
    "    sample_f1 = f1_score(y_true=val_labels, y_pred=1 * (val_outs > 0), average=\"samples\")  # mean f1\n",
    "\n",
    "    if len(val_sample_f1) == 0 or sample_f1 > max(val_sample_f1):\n",
    "        print(\"saving best\")\n",
    "        try:\n",
    "            del model_in_memory[\"best\"]\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        model.to(\"cpu\")\n",
    "        model_in_memory[\"best\"] = deepcopy(model.state_dict())\n",
    "        model.to(device)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model_in_memory[\"best\"], \"test_v7.5.pt\")\n",
    "\n",
    "    val_mean_f1.append(mean_f1)\n",
    "    val_sample_f1.append(sample_f1)\n",
    "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\n",
    "        f\"Epoch:{epoch}, train/val loss:{round(np.mean(train_loss),7),round(np.mean(val_loss),7)},sample/mean f1:{round(sample_f1, 7), round(mean_f1, 7)}, lr:{cur_lr}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effv2 + tiny (99.9mb) finetune from 0; 5.12, 5e-05: 85.7\n",
    "# effv2 + tiny (99.9mb) finetune from 0; 5.7, 5e-05: 85.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_in_memory[\"best\"], \"test_v7.5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopper for run all\n",
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Combined_Model(features, lang_model)\n",
    "model.load_state_dict(torch.load(\"test_v7.5.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cheat / or use val dataset. Want\n",
    "class CheatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pd.read_csv(\"test_cheat.csv\", names=range(3), skiprows=1)[[0, 2, 1]].dropna()\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 2: \"label\", 1: \"caption\"}, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        label = self.df_data.iloc[idx, 1]\n",
    "        caption = self.df_data.iloc[idx, 2]\n",
    "\n",
    "        sample = {\"caption\": caption, \"label\": label, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "cheat_data = CheatDataset(transform=transforms)\n",
    "# vocabularise_caption(train_data, vocab)\n",
    "one_hot_encode_labels(cheat_data)\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# train_data.__len__() == 30000\n",
    "cheat_dl = DataLoader(\n",
    "    cheat_data, batch_size=8, shuffle=False, collate_fn=train_collate_fn, num_workers=24\n",
    ")\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# use the same validation set/training set\n",
    "val_outs = []\n",
    "val_labels = []\n",
    "model.eval()\n",
    "\n",
    "for i, batch in enumerate(cheat_dl):\n",
    "\n",
    "    captions = batch[\"caption\"]\n",
    "    images = batch[\"image\"].to(DEVICE)\n",
    "    labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "    inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # predictions = model(images, captions)\n",
    "    predictions = model(images, input_ids, attention_mask)\n",
    "\n",
    "    val_outs.append(predictions.detach().cpu().numpy())\n",
    "    val_labels.append(labels.detach().cpu().numpy())\n",
    "val_labels = np.vstack(val_labels)\n",
    "val_outs = np.vstack(val_outs)\n",
    "\n",
    "\n",
    "best_thresholds = np.zeros(18)\n",
    "for i in range(18):\n",
    "    fpr, tpr, thresholds = precision, recall, thresholds = precision_recall_curve(\n",
    "        val_labels[:, i], (val_outs)[:, i]\n",
    "    )\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    best_thresholds[i] = thresholds[ix]\n",
    "\n",
    "print(best_thresholds)\n",
    "np.save(\"best_thresholds\", best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=val_labels, y_pred=1.0 * (val_outs > 0), average=\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=val_labels, y_pred=1.0 * (val_outs - best_thresholds > 0), average=\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopper for run all\n",
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_collate_fn(X):\n",
    "    # convert [{key: val, ...}, ...]\n",
    "    # to [key: [val, ...],  ...}\n",
    "    X = {k: [v[k] for v in X] for k in X[0]}\n",
    "    X[\"image\"] = torch.stack(X[\"image\"])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "test_data = TestDataset(transform=transforms)\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=16, shuffle=False, collate_fn=test_collate_fn, num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "# save predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "for i, batch in enumerate(test_dl):\n",
    "    captions = batch[\"caption\"]\n",
    "    images = batch[\"image\"].to(DEVICE)\n",
    "\n",
    "    inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    predictions = model(images, input_ids, attention_mask)\n",
    "    test_preds.append(predictions.detach().cpu().numpy())\n",
    "\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "\n",
    "def out_logits_to_preds(logits, best_thresholds):\n",
    "    labels = []\n",
    "    logits = (logits - best_thresholds) > 0\n",
    "    lables_available = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19])\n",
    "    for i in range(logits.shape[0]):\n",
    "        labels.append(list(lables_available[logits[i]]))\n",
    "    return labels\n",
    "\n",
    "\n",
    "# lables_available[(test_preds[:10] > 0)]\n",
    "# {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19}\n",
    "\n",
    "test_labels = out_logits_to_preds(test_preds, best_thresholds)\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH, names=range(3), skiprows=1)\n",
    "test_labels_str = [\" \".join([str(i) for i in labels]) for labels in test_labels]\n",
    "df_test[\"Labels\"] = test_labels_str\n",
    "df_test.rename({0: \"ImageID\"}, axis=1, inplace=True)\n",
    "\n",
    "df_test[[\"ImageID\", \"Labels\"]].to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "COMP5329_Assignment_2_v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "105bdd467ce44751b3020eddd3d4c791": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37766d0f97754f21a9b78ec7c2c9b266": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b5faf89e145490b8df3c5a0974a30e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37766d0f97754f21a9b78ec7c2c9b266",
      "max": 21388428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0e1d4db2eb74c24887d9b9d63e36a2a",
      "value": 21388428
     }
    },
    "6c592ed619724f1997199bf45b9727e7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775c2d8bc3824da89165a6baf24327c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b5faf89e145490b8df3c5a0974a30e0",
       "IPY_MODEL_fbc0ba310f384411ad658bc8ceee5607"
      ],
      "layout": "IPY_MODEL_6c592ed619724f1997199bf45b9727e7"
     }
    },
    "9929ab1374f74db7addc12be1eaa6a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0e1d4db2eb74c24887d9b9d63e36a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fbc0ba310f384411ad658bc8ceee5607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_105bdd467ce44751b3020eddd3d4c791",
      "placeholder": "​",
      "style": "IPY_MODEL_9929ab1374f74db7addc12be1eaa6a29",
      "value": " 20.4M/20.4M [00:00&lt;00:00, 70.0MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
