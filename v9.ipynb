{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYTGkadIZh4_"
   },
   "source": [
    "# COMP5329 - Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1621175101439,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "_HHNTgKRLXK2",
    "outputId": "3c6183f6-fc17-4847-f133-296f04a53d47"
   },
   "outputs": [],
   "source": [
    "# import google\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import torchtext\n",
    "import PIL.Image\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# MOUNT_PATH = '/content/drive'\n",
    "# DRIVE_PATH = f'{MOUNT_PATH}/My Drive'\n",
    "# PROJECT_PATH = DRIVE_PATH + \"/Assignment 2\"\n",
    "PROJECT_PATH = \"./\"\n",
    "IMG_PATH = f\"{PROJECT_PATH}/data\"\n",
    "TRAIN_CSV_PATH = f\"{PROJECT_PATH}/train.csv\"\n",
    "TEST_CSV_PATH = f\"{PROJECT_PATH}/test.csv\"\n",
    "\n",
    "# google.colab.drive.mount(MOUNT_PATH)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"efficientdet-d0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from effdet import DetBenchTrain, EfficientDet, get_efficientdet_config\n",
    "# from effdet.efficientdet import HeadNet\n",
    "\n",
    "\n",
    "# def get_net():\n",
    "#     config = get_efficientdet_config(\"tf_efficientdet_d0\")\n",
    "#     net = EfficientDet(config, pretrained_backbone=True)\n",
    "#     #     checkpoint = torch.load('../input/efficientdet/efficientdet_d5-ef44aea8.pth')\n",
    "#     #     net.load_state_dict(checkpoint)\n",
    "#     num_outputs = 18\n",
    "#     config.image_size = 224\n",
    "#     net.class_net = HeadNet(config, num_outputs=18, norm_kwargs=dict(eps=0.001, momentum=0.01))\n",
    "#     return DetBenchTrain(net, config)\n",
    "\n",
    "\n",
    "# # model = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = get_efficientdet_config(\"tf_efficientdet_d0\")\n",
    "\n",
    "config.image_size = [256, 256]\n",
    "config.norm_kwargs = dict(eps=0.001, momentum=0.01)\n",
    "\n",
    "model = EfficientDet(config, pretrained_backbone=True)\n",
    "# checkpoint = torch.load('../input/efficientdet/efficientdet_d5-ef44aea8.pth')\n",
    "# net.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backbone', 'fpn', 'class_net', 'box_net']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(name for name, _ in model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backbone', 'fpn', 'class_net', 'box_net']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(name for name, _ in model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# img = torch.randn(1, 3, 224, 224)\n",
    "# model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=18)\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "# summary(model, input_data=img, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from effdet import create_model\n",
    "\n",
    "# model = create_model(\"tf_efficientdet_d0\", bench_task=\"predict\", num_classes=18, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 112, 16, 16])\n",
      "torch.Size([1, 320, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "features = model.backbone(img)\n",
    "for i in features:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40, 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 64, 16, 16])\n",
      "torch.Size([1, 64, 8, 8])\n",
      "torch.Size([1, 64, 4, 4])\n",
      "torch.Size([1, 64, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fpn_out = model.fpn(features)\n",
    "for i in fpn_out:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 810, 32, 32])\n",
      "torch.Size([1, 810, 16, 16])\n",
      "torch.Size([1, 810, 8, 8])\n",
      "torch.Size([1, 810, 4, 4])\n",
      "torch.Size([1, 810, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "class_out = model.class_net(fpn_out)\n",
    "for i in class_out:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[[-4.3960, -4.3962, -4.2239,  ..., -4.1705, -4.3907, -4.4360],\n",
       "            [-4.3503, -4.2365, -4.2250,  ..., -4.2904, -4.5557, -4.9295],\n",
       "            [-4.5122, -4.0054, -4.3810,  ..., -4.5231, -4.6471, -4.8485],\n",
       "            ...,\n",
       "            [-4.6010, -4.5866, -4.4898,  ..., -4.3715, -4.2727, -4.4382],\n",
       "            [-4.5806, -4.6248, -4.8934,  ..., -4.3190, -4.4208, -4.5099],\n",
       "            [-4.5644, -4.7300, -4.6838,  ..., -4.3797, -4.3676, -4.6539]],\n",
       "  \n",
       "           [[-4.4077, -4.7504, -4.7537,  ..., -5.0152, -4.8749, -4.5994],\n",
       "            [-4.4385, -4.5124, -4.1896,  ..., -4.9844, -4.6363, -4.3155],\n",
       "            [-4.4624, -5.1129, -4.3522,  ..., -4.7806, -4.8280, -4.5345],\n",
       "            ...,\n",
       "            [-4.4559, -4.5407, -4.4826,  ..., -4.7122, -4.5267, -4.4566],\n",
       "            [-4.4663, -4.6158, -4.7068,  ..., -4.8090, -4.7862, -4.8100],\n",
       "            [-4.6702, -4.4927, -4.7266,  ..., -4.6503, -4.2228, -4.4610]],\n",
       "  \n",
       "           [[-4.3861, -4.3784, -4.7552,  ..., -4.5204, -4.6573, -4.6912],\n",
       "            [-4.8042, -4.7675, -5.2987,  ..., -4.6938, -4.9946, -4.6278],\n",
       "            [-4.7669, -4.7439, -4.8537,  ..., -5.1617, -4.6317, -4.2861],\n",
       "            ...,\n",
       "            [-4.5844, -4.5115, -4.5147,  ..., -5.0127, -4.7813, -4.8106],\n",
       "            [-4.5549, -4.1745, -4.6917,  ..., -4.7205, -4.9591, -4.5363],\n",
       "            [-4.7186, -4.4627, -4.7768,  ..., -4.6359, -5.1436, -4.7768]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.7716, -4.3032, -4.5533,  ..., -4.5213, -4.4384, -4.5274],\n",
       "            [-4.6879, -4.3059, -4.9422,  ..., -4.1248, -4.1093, -4.6661],\n",
       "            [-4.8216, -3.9838, -4.4839,  ..., -3.9582, -4.2662, -4.6517],\n",
       "            ...,\n",
       "            [-4.6226, -4.4131, -5.0171,  ..., -3.7984, -4.6323, -4.5197],\n",
       "            [-4.5557, -3.9161, -4.2075,  ..., -3.7999, -4.3503, -4.5061],\n",
       "            [-4.3440, -4.3344, -4.5609,  ..., -3.9326, -4.4502, -4.3611]],\n",
       "  \n",
       "           [[-4.4701, -4.5228, -4.7813,  ..., -4.3982, -3.9212, -4.4359],\n",
       "            [-4.2249, -4.1623, -4.4779,  ..., -4.3403, -3.8429, -4.4050],\n",
       "            [-4.2837, -4.1162, -4.3603,  ..., -4.4858, -3.9139, -4.6068],\n",
       "            ...,\n",
       "            [-4.1747, -4.1650, -4.3962,  ..., -4.2866, -4.2315, -4.5219],\n",
       "            [-4.3833, -3.9359, -4.0171,  ..., -4.2420, -4.0973, -4.3804],\n",
       "            [-4.2288, -4.3496, -4.3411,  ..., -4.2717, -4.4441, -4.2559]],\n",
       "  \n",
       "           [[-4.6848, -4.6409, -4.8160,  ..., -4.7058, -4.8428, -4.6115],\n",
       "            [-4.6496, -4.7267, -4.9084,  ..., -4.8560, -4.6325, -4.4742],\n",
       "            [-4.7760, -4.5950, -4.8050,  ..., -4.7879, -4.5124, -4.3005],\n",
       "            ...,\n",
       "            [-4.6850, -4.7355, -4.7708,  ..., -5.1867, -4.9791, -4.5901],\n",
       "            [-4.8641, -4.7203, -4.5522,  ..., -5.0258, -4.9346, -4.8221],\n",
       "            [-4.8389, -4.7829, -4.4214,  ..., -4.6917, -4.7184, -4.6025]]]],\n",
       "         grad_fn=<MkldnnConvolutionBackward>),\n",
       "  tensor([[[[-4.4743, -4.7393, -4.4965,  ..., -4.5428, -4.6172, -4.6557],\n",
       "            [-4.5518, -4.3376, -4.8740,  ..., -4.2827, -4.6025, -4.4433],\n",
       "            [-4.6051, -4.4211, -4.2312,  ..., -4.5112, -4.6049, -4.5164],\n",
       "            ...,\n",
       "            [-4.7930, -4.4121, -4.7371,  ..., -4.6269, -4.4675, -4.6983],\n",
       "            [-4.4157, -4.3034, -5.0112,  ..., -4.3228, -4.7346, -4.5640],\n",
       "            [-4.6061, -4.8991, -5.0844,  ..., -4.5345, -4.5447, -4.6251]],\n",
       "  \n",
       "           [[-4.4325, -4.4108, -4.7223,  ..., -4.4271, -4.4902, -4.4633],\n",
       "            [-4.5353, -3.9981, -3.1789,  ..., -4.6237, -4.5470, -4.8864],\n",
       "            [-3.5397, -4.3412, -4.5454,  ..., -4.2577, -4.6346, -4.5633],\n",
       "            ...,\n",
       "            [-4.0365, -4.0073, -5.1376,  ..., -4.8179, -4.3485, -4.5633],\n",
       "            [-4.5485, -4.5978, -4.5875,  ..., -4.9940, -4.4796, -4.8937],\n",
       "            [-4.1802, -4.7430, -4.9506,  ..., -4.2908, -4.6466, -4.6915]],\n",
       "  \n",
       "           [[-4.5946, -4.8234, -4.5620,  ..., -4.8165, -4.6125, -4.4234],\n",
       "            [-4.5945, -5.7469, -5.3858,  ..., -4.5557, -4.8467, -4.5748],\n",
       "            [-4.9509, -4.9301, -4.4844,  ..., -4.7970, -4.5824, -4.5050],\n",
       "            ...,\n",
       "            [-5.0832, -4.9572, -4.3628,  ..., -3.8544, -5.0150, -4.8052],\n",
       "            [-4.5952, -4.6596, -5.1622,  ..., -4.9010, -4.5608, -4.8061],\n",
       "            [-4.6347, -4.4346, -4.4091,  ..., -4.4432, -4.6840, -4.8743]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.3356, -4.3920, -5.0794,  ..., -4.4217, -4.3830, -4.6285],\n",
       "            [-4.4952, -5.1602, -4.8624,  ..., -4.8448, -4.5168, -4.6142],\n",
       "            [-4.1435, -4.6410, -3.8325,  ..., -3.9278, -4.1069, -4.6597],\n",
       "            ...,\n",
       "            [-4.9248, -4.2974, -4.0823,  ..., -3.2577, -4.0905, -4.2603],\n",
       "            [-4.5621, -4.2799, -4.4982,  ..., -3.8553, -4.0804, -4.3165],\n",
       "            [-4.4567, -4.2396, -4.0201,  ..., -3.8000, -4.2954, -4.3885]],\n",
       "  \n",
       "           [[-4.2302, -4.7481, -4.4133,  ..., -4.3759, -4.5315, -4.7226],\n",
       "            [-3.9629, -4.6148, -4.2249,  ..., -4.4908, -4.4177, -4.4672],\n",
       "            [-4.2726, -4.8771, -5.0552,  ..., -4.3177, -4.8563, -4.2770],\n",
       "            ...,\n",
       "            [-4.1552, -4.2481, -3.7842,  ..., -3.4280, -4.2517, -4.3992],\n",
       "            [-4.0828, -4.3628, -3.7170,  ..., -3.7049, -4.6264, -4.2831],\n",
       "            [-4.0736, -4.2486, -4.1338,  ..., -3.7737, -4.3060, -4.4093]],\n",
       "  \n",
       "           [[-4.8318, -4.4107, -4.5680,  ..., -4.5103, -4.4387, -4.4007],\n",
       "            [-4.7453, -4.4022, -3.9301,  ..., -4.7368, -4.4962, -4.5186],\n",
       "            [-4.3825, -4.8406, -4.3850,  ..., -4.5504, -4.8248, -4.6729],\n",
       "            ...,\n",
       "            [-4.7246, -4.6006, -4.6843,  ..., -5.3454, -5.1399, -4.6412],\n",
       "            [-4.4061, -4.9853, -5.0523,  ..., -4.5621, -4.9704, -4.5366],\n",
       "            [-4.6870, -4.7260, -4.6835,  ..., -4.3978, -4.6880, -4.5914]]]],\n",
       "         grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[-4.5595, -4.3663, -4.2479,  ..., -3.9963, -3.9530, -4.2461],\n",
       "            [-4.5035, -4.5208, -4.5489,  ..., -4.0363, -4.1940, -4.4754],\n",
       "            [-4.9427, -5.2905, -4.4614,  ..., -4.3141, -5.1818, -5.2684],\n",
       "            ...,\n",
       "            [-4.9863, -5.4700, -4.6079,  ..., -4.8286, -4.1170, -3.9172],\n",
       "            [-5.0224, -4.5831, -5.3775,  ..., -4.8419, -4.5083, -4.5055],\n",
       "            [-4.7669, -4.0069, -4.6016,  ..., -4.5854, -4.7468, -4.7701]],\n",
       "  \n",
       "           [[-4.5870, -4.9066, -4.5269,  ..., -4.8467, -4.9926, -4.3245],\n",
       "            [-4.8867, -4.9470, -4.6287,  ..., -3.9524, -5.1803, -4.3198],\n",
       "            [-4.8555, -4.0771, -3.3068,  ..., -4.0043, -4.5537, -4.6794],\n",
       "            ...,\n",
       "            [-4.5865, -4.2970, -5.6960,  ..., -4.5818, -5.1267, -5.0283],\n",
       "            [-4.4458, -3.9242, -4.5534,  ..., -4.6591, -5.3711, -4.8759],\n",
       "            [-4.4672, -4.7436, -5.0638,  ..., -4.6823, -4.4928, -4.3489]],\n",
       "  \n",
       "           [[-4.2975, -4.6870, -5.0049,  ..., -5.5404, -5.0299, -4.9897],\n",
       "            [-4.7547, -5.2104, -4.7591,  ..., -5.4721, -5.6359, -4.3775],\n",
       "            [-4.9097, -5.2281, -5.0813,  ..., -6.0496, -5.3193, -3.7087],\n",
       "            ...,\n",
       "            [-4.9968, -5.3189, -4.8859,  ..., -3.6753, -5.2953, -5.2579],\n",
       "            [-4.6338, -4.2769, -5.3372,  ..., -4.6727, -4.8152, -4.7215],\n",
       "            [-4.6638, -5.2055, -4.2671,  ..., -4.4176, -4.8056, -4.7632]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.8057, -4.6764, -4.6067,  ..., -4.0179, -4.4109, -4.4518],\n",
       "            [-4.5278, -4.8900, -4.1086,  ..., -4.7137, -4.7158, -4.4638],\n",
       "            [-3.8589, -4.7964, -3.9778,  ..., -4.8022, -3.6981, -4.1934],\n",
       "            ...,\n",
       "            [-4.7211, -4.6245, -5.7170,  ..., -4.2775, -5.0283, -4.4933],\n",
       "            [-4.8002, -4.6535, -4.7150,  ..., -4.5458, -4.2662, -4.2962],\n",
       "            [-4.2183, -4.2603, -4.1101,  ..., -4.3996, -4.2430, -4.4550]],\n",
       "  \n",
       "           [[-4.1900, -4.4461, -4.5286,  ..., -4.4077, -4.7931, -4.9281],\n",
       "            [-3.9532, -4.4282, -4.0179,  ..., -4.2928, -4.2531, -4.9268],\n",
       "            [-4.2429, -4.5875, -4.7860,  ..., -4.8501, -4.3451, -4.6693],\n",
       "            ...,\n",
       "            [-4.2333, -4.8870, -4.4348,  ..., -5.2425, -5.6197, -4.4547],\n",
       "            [-3.9736, -3.8963, -3.2742,  ..., -3.2154, -3.8874, -4.2364],\n",
       "            [-4.5021, -4.9394, -4.0084,  ..., -4.2162, -4.6942, -4.7664]],\n",
       "  \n",
       "           [[-4.9369, -4.5151, -5.1471,  ..., -4.8939, -4.9222, -4.6714],\n",
       "            [-4.4947, -4.1400, -5.1535,  ..., -4.6911, -4.4352, -4.9881],\n",
       "            [-4.9235, -3.7602, -4.2733,  ..., -4.9870, -4.5664, -4.5254],\n",
       "            ...,\n",
       "            [-4.7853, -4.6262, -4.4875,  ..., -5.0857, -5.2673, -4.5866],\n",
       "            [-4.8740, -5.1474, -4.1918,  ..., -5.0563, -5.2633, -5.2276],\n",
       "            [-4.3429, -4.0947, -4.5143,  ..., -4.5766, -4.2988, -4.6287]]]],\n",
       "         grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[-4.6561, -4.0678, -3.8831, -5.0121],\n",
       "            [-4.7565, -4.1271, -4.7308, -4.1914],\n",
       "            [-4.5823, -4.9198, -4.9007, -4.8362],\n",
       "            [-4.5721, -4.8922, -4.4929, -4.8755]],\n",
       "  \n",
       "           [[-4.0235, -4.1693, -5.2595, -3.8729],\n",
       "            [-4.3880, -4.8905, -3.9791, -5.5629],\n",
       "            [-5.2665, -4.6460, -4.0102, -4.4635],\n",
       "            [-5.3030, -5.7743, -4.9774, -4.6815]],\n",
       "  \n",
       "           [[-4.8777, -5.6558, -5.4413, -4.5569],\n",
       "            [-4.1616, -5.0790, -5.4536, -5.7527],\n",
       "            [-5.2536, -4.2010, -4.0720, -5.5125],\n",
       "            [-4.7775, -3.0864, -4.3611, -4.6648]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.6940, -4.8584, -4.5342, -4.6158],\n",
       "            [-4.9270, -4.7638, -5.2487, -5.2232],\n",
       "            [-3.9880, -4.3645, -4.5241, -4.6249],\n",
       "            [-4.8818, -4.8658, -3.7135, -3.9765]],\n",
       "  \n",
       "           [[-4.2789, -4.3202, -4.3836, -4.8680],\n",
       "            [-4.2969, -4.7874, -5.5127, -4.8074],\n",
       "            [-3.8783, -3.9909, -3.7163, -4.2500],\n",
       "            [-3.5553, -3.9233, -4.1699, -4.1366]],\n",
       "  \n",
       "           [[-4.8194, -4.3986, -4.7498, -4.8193],\n",
       "            [-4.6063, -5.3340, -4.1687, -4.0812],\n",
       "            [-3.8352, -4.8437, -4.7028, -4.4123],\n",
       "            [-4.8576, -5.2171, -5.1814, -4.8280]]]],\n",
       "         grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[-4.6395, -4.3456],\n",
       "            [-4.3193, -4.9984]],\n",
       "  \n",
       "           [[-4.2604, -4.2714],\n",
       "            [-4.7151, -4.9319]],\n",
       "  \n",
       "           [[-5.4229, -4.8559],\n",
       "            [-4.8306, -4.5247]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-4.5128, -5.0978],\n",
       "            [-5.0842, -4.4068]],\n",
       "  \n",
       "           [[-4.5590, -4.5957],\n",
       "            [-4.2635, -4.4310]],\n",
       "  \n",
       "           [[-4.1362, -5.0184],\n",
       "            [-4.8564, -4.5822]]]], grad_fn=<ThnnConv2DBackward>)],\n",
       " [tensor([[[[-2.5383e-01, -1.3741e-01, -3.5743e-02,  ..., -2.3753e-01,\n",
       "             -1.0988e-01, -1.4341e-01],\n",
       "            [-1.4166e-01,  1.4569e-02,  1.3660e-02,  ..., -2.1593e-01,\n",
       "             -2.4134e-02, -6.9493e-02],\n",
       "            [-8.2372e-02,  7.2072e-02,  1.6948e-01,  ..., -1.1181e-01,\n",
       "             -8.0961e-02, -9.0080e-03],\n",
       "            ...,\n",
       "            [-1.5439e-01, -3.3636e-01, -3.5298e-01,  ..., -2.9978e-01,\n",
       "             -7.5599e-01, -2.8269e-01],\n",
       "            [-4.4031e-01, -6.3762e-02, -2.6050e-01,  ..., -2.5749e-01,\n",
       "             -4.5649e-01, -5.3587e-01],\n",
       "            [-1.4589e-01, -5.6027e-02,  7.2604e-02,  ..., -2.1196e-01,\n",
       "             -5.5696e-01, -2.3286e-01]],\n",
       "  \n",
       "           [[-1.7766e-01, -1.4961e-01, -8.6867e-02,  ..., -2.4381e-01,\n",
       "              6.2782e-02, -2.8072e-03],\n",
       "            [ 3.4836e-02, -4.2099e-02, -1.8337e-01,  ..., -1.7370e-01,\n",
       "             -4.5000e-02,  1.8508e-01],\n",
       "            [ 1.4063e-01, -3.0854e-03,  2.0876e-01,  ...,  3.9325e-01,\n",
       "             -8.1524e-02, -4.4078e-02],\n",
       "            ...,\n",
       "            [ 8.5911e-02, -2.9614e-01, -2.3710e-01,  ..., -2.2925e-01,\n",
       "             -3.1743e-01,  3.9503e-02],\n",
       "            [-8.9527e-02,  9.8256e-02, -6.1736e-02,  ..., -1.1525e-01,\n",
       "             -8.9989e-02, -5.7204e-02],\n",
       "            [ 1.0813e-01, -4.2370e-02, -2.9575e-01,  ..., -1.5596e-01,\n",
       "             -1.8980e-01, -2.6144e-01]],\n",
       "  \n",
       "           [[-7.7467e-03, -1.1842e-02,  4.2115e-02,  ...,  2.2059e-02,\n",
       "              2.6198e-01, -2.1258e-01],\n",
       "            [-1.0928e-01, -1.3864e-01, -1.4118e-01,  ..., -2.6054e-02,\n",
       "              1.6240e-01,  1.3863e-01],\n",
       "            [ 1.1562e-01, -8.7252e-02,  1.1374e-01,  ...,  1.5571e-01,\n",
       "             -3.4792e-01, -1.3071e-01],\n",
       "            ...,\n",
       "            [-3.4089e-01,  1.9612e-01,  1.9486e-01,  ...,  4.6039e-01,\n",
       "              3.6078e-01,  1.6877e-01],\n",
       "            [-9.8683e-02,  2.4959e-02,  2.0477e-01,  ...,  3.8912e-01,\n",
       "              3.5366e-01,  1.5363e-01],\n",
       "            [-2.4533e-02,  3.0018e-02, -1.3482e-01,  ..., -3.4818e-03,\n",
       "              7.1076e-02, -5.2064e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 9.2208e-02, -2.2602e-01, -1.4212e-01,  ..., -2.0498e-01,\n",
       "             -1.0336e-01,  1.0951e-01],\n",
       "            [-2.5874e-01, -3.2211e-01, -2.0102e-01,  ..., -2.0094e-01,\n",
       "             -3.2546e-01, -3.6796e-01],\n",
       "            [-3.2365e-01, -1.7499e-01, -5.9707e-01,  ..., -5.6082e-01,\n",
       "             -1.3756e-01, -1.3970e-01],\n",
       "            ...,\n",
       "            [ 1.7706e-01, -1.7996e-01, -6.6703e-01,  ..., -8.1750e-01,\n",
       "             -3.5127e-01, -6.9036e-03],\n",
       "            [-1.3473e-02, -2.3164e-01, -3.5973e-01,  ..., -3.8320e-01,\n",
       "             -4.8471e-01, -5.2442e-02],\n",
       "            [ 6.5109e-02,  2.1205e-02,  7.1448e-03,  ...,  2.8020e-01,\n",
       "              3.7862e-02,  2.5698e-01]],\n",
       "  \n",
       "           [[-8.9945e-02,  2.6156e-01, -1.9942e-02,  ...,  9.9078e-02,\n",
       "              3.1862e-01,  1.9930e-01],\n",
       "            [-5.2435e-02,  6.2886e-01,  2.1520e-01,  ..., -5.3413e-03,\n",
       "              1.2251e-01,  5.4655e-01],\n",
       "            [-2.3081e-01,  4.7643e-01, -1.6307e-01,  ...,  4.4036e-02,\n",
       "              2.3112e-01,  1.3212e-01],\n",
       "            ...,\n",
       "            [ 2.7004e-01,  1.5672e-01, -7.8250e-01,  ..., -3.2658e-01,\n",
       "             -3.4698e-01, -1.4541e-01],\n",
       "            [ 1.7155e-01,  2.8546e-01, -1.9767e-01,  ...,  7.9117e-02,\n",
       "             -5.5556e-01, -1.0232e-01],\n",
       "            [ 9.3133e-02,  7.9573e-03, -2.7676e-01,  ...,  2.0308e-01,\n",
       "             -1.9329e-01, -1.4036e-01]],\n",
       "  \n",
       "           [[-6.5818e-02,  1.5450e-01,  4.8242e-01,  ..., -3.0788e-02,\n",
       "             -9.7313e-02, -3.9695e-02],\n",
       "            [ 5.0373e-02,  2.4706e-01,  1.1501e-01,  ...,  8.1766e-02,\n",
       "              3.6664e-02,  1.6387e-01],\n",
       "            [ 9.1723e-02,  2.4473e-01,  1.1739e-01,  ..., -1.6681e-01,\n",
       "             -3.7410e-01, -3.3492e-01],\n",
       "            ...,\n",
       "            [ 4.0277e-01,  4.3585e-01,  1.8323e-01,  ...,  1.3715e-01,\n",
       "             -7.2188e-02,  2.3071e-01],\n",
       "            [ 4.0833e-01,  1.7114e-01, -9.2649e-02,  ..., -1.6802e-01,\n",
       "             -2.4075e-01, -6.3061e-02],\n",
       "            [ 6.8670e-02, -4.2107e-04,  3.8467e-01,  ..., -2.2055e-01,\n",
       "             -1.9768e-01,  3.9333e-02]]]], grad_fn=<MkldnnConvolutionBackward>),\n",
       "  tensor([[[[-0.2395, -0.3377,  0.0189,  ..., -0.0725, -0.2139, -0.4809],\n",
       "            [ 0.2828,  0.1714,  0.3021,  ...,  0.3889,  0.4789, -0.2067],\n",
       "            [-0.1322,  0.7127,  0.2264,  ...,  0.6824,  0.0125, -0.1909],\n",
       "            ...,\n",
       "            [ 0.0534,  1.0070, -0.2483,  ..., -0.6565,  0.0346, -0.6053],\n",
       "            [-0.2937, -0.0294, -0.6365,  ...,  0.4181, -0.0768,  0.0736],\n",
       "            [-0.0337,  0.0653, -0.5121,  ..., -0.0937,  0.2279,  0.1148]],\n",
       "  \n",
       "           [[-0.3621, -0.4219, -0.1375,  ..., -0.1582, -0.1497,  0.0910],\n",
       "            [ 0.0123, -1.2476, -0.7708,  ...,  0.1457, -0.1596,  0.0259],\n",
       "            [ 0.0577,  0.2931, -0.8044,  ...,  0.0514, -0.0090, -0.0890],\n",
       "            ...,\n",
       "            [ 0.1726,  0.5390, -0.2469,  ...,  0.5506, -0.5667,  0.0479],\n",
       "            [-0.4580,  0.2102,  0.1301,  ..., -0.0736, -0.3104, -0.1660],\n",
       "            [-0.6742, -0.5065, -0.0137,  ..., -0.4301, -0.2490, -0.1871]],\n",
       "  \n",
       "           [[ 0.1485, -0.0312,  0.0667,  ..., -0.2114,  0.0442, -0.1673],\n",
       "            [-0.0270, -0.7213, -0.1649,  ..., -0.3609, -0.4197, -0.4936],\n",
       "            [-0.4791, -0.2784, -0.2314,  ..., -0.4437,  0.0092,  0.0078],\n",
       "            ...,\n",
       "            [-0.2853, -0.8645, -0.5215,  ..., -0.1151, -0.0273, -0.1356],\n",
       "            [ 0.1632, -0.2791, -0.4059,  ..., -0.1847, -0.1689, -0.1501],\n",
       "            [ 0.2718, -0.4357, -0.3785,  ...,  0.1293, -0.0998, -0.2733]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1376, -0.2486,  0.0929,  ..., -0.1367, -0.0082,  0.4793],\n",
       "            [ 0.0542,  0.2272,  0.4306,  ..., -0.9484, -0.4651,  0.3597],\n",
       "            [-0.1503, -0.5314, -0.8432,  ...,  0.2324,  0.0722,  0.2776],\n",
       "            ...,\n",
       "            [ 0.2811,  0.0719, -0.4937,  ...,  0.0047,  0.2200, -0.0728],\n",
       "            [ 0.1716,  0.4464, -0.4903,  ...,  0.0311,  0.1433,  0.1965],\n",
       "            [-0.1015,  0.2046, -0.0393,  ...,  0.1814, -0.2226,  0.0215]],\n",
       "  \n",
       "           [[-0.1603,  0.1173, -0.0102,  ..., -0.1345,  0.0477,  0.3708],\n",
       "            [ 0.0484, -0.3952, -0.1433,  ..., -0.0933, -0.1334,  0.0966],\n",
       "            [-0.3139,  0.0783, -0.2581,  ..., -0.2589,  0.2833,  0.2981],\n",
       "            ...,\n",
       "            [-0.1964,  0.6303, -0.5066,  ...,  0.0559, -1.0281, -0.2811],\n",
       "            [-0.2551,  0.3055, -0.5112,  ...,  0.6212,  0.2277, -0.0261],\n",
       "            [-0.1394, -0.0199,  0.1390,  ..., -0.4390,  0.3769,  0.1744]],\n",
       "  \n",
       "           [[ 0.3262, -0.5163, -0.2476,  ..., -0.0662,  0.0512, -0.1079],\n",
       "            [-0.0285, -0.4525, -0.1724,  ...,  0.0966,  0.1303,  0.1466],\n",
       "            [-0.3007,  0.2728, -0.9708,  ...,  0.0851,  0.0296, -0.3762],\n",
       "            ...,\n",
       "            [-0.2827,  0.4845,  0.1387,  ...,  0.3537,  0.0171, -0.2202],\n",
       "            [ 0.0189, -0.1465, -0.0029,  ...,  0.6747,  0.6506,  0.1233],\n",
       "            [ 0.3047,  0.2216, -0.0820,  ..., -0.3917,  0.4199,  0.3246]]]],\n",
       "         grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[ 7.8657e-01,  7.3698e-01,  9.0164e-01,  ..., -2.5418e-01,\n",
       "              4.0991e-03, -3.4506e-02],\n",
       "            [ 5.3724e-01,  3.4975e-01,  2.4209e-01,  ...,  1.4770e-01,\n",
       "             -2.1429e-01,  1.3906e-01],\n",
       "            [-2.2082e-01,  2.0381e-01, -8.9558e-02,  ...,  4.1256e-01,\n",
       "              4.3045e-01, -3.3658e-01],\n",
       "            ...,\n",
       "            [ 3.4730e-01,  4.1716e-01, -1.4488e-01,  ..., -7.9288e-01,\n",
       "             -2.3520e-01, -4.0924e-02],\n",
       "            [-2.1814e-01, -1.7772e-01, -3.0999e-01,  ...,  2.1907e-01,\n",
       "             -9.9181e-01, -7.8252e-01],\n",
       "            [-2.9343e-01, -6.6432e-02,  7.9014e-02,  ...,  4.2537e-01,\n",
       "              1.1827e-01, -1.4500e-01]],\n",
       "  \n",
       "           [[ 3.3045e-01, -4.8852e-01,  2.6139e-01,  ...,  1.2258e-01,\n",
       "             -1.8910e-01, -4.5203e-02],\n",
       "            [ 2.9263e-01, -7.8946e-01, -6.2294e-01,  ..., -3.9039e-01,\n",
       "             -9.2150e-01,  4.6687e-02],\n",
       "            [ 3.8151e-01, -2.4702e-01, -1.2857e+00,  ..., -3.6085e-02,\n",
       "             -3.2283e-01, -1.5046e-01],\n",
       "            ...,\n",
       "            [ 4.7464e-01,  2.4923e-01, -7.3552e-01,  ...,  1.8632e-02,\n",
       "             -5.4577e-01,  3.2062e-01],\n",
       "            [ 1.7792e-02, -5.3358e-02, -2.7642e-01,  ..., -3.9692e-01,\n",
       "             -9.8463e-02, -4.7664e-01],\n",
       "            [ 1.2065e-01, -2.0870e-01, -4.0315e-02,  ..., -3.0478e-01,\n",
       "             -1.1198e-01,  1.4773e-01]],\n",
       "  \n",
       "           [[-2.5636e-01, -6.0326e-01, -5.1396e-01,  ..., -3.8323e-01,\n",
       "             -2.8734e-01, -2.7440e-01],\n",
       "            [-5.1550e-01, -4.5646e-03, -6.2286e-01,  ..., -3.7317e-01,\n",
       "             -2.5494e-01, -2.2010e-01],\n",
       "            [-8.6308e-02,  1.4582e-01, -8.6181e-01,  ..., -8.5062e-01,\n",
       "             -1.9351e-02, -5.5734e-01],\n",
       "            ...,\n",
       "            [ 5.8303e-01,  3.6021e-01, -1.2129e+00,  ..., -5.0400e-01,\n",
       "             -3.2397e-01, -1.3159e-01],\n",
       "            [ 2.0271e-01,  8.7732e-03, -6.2809e-02,  ...,  2.3007e-02,\n",
       "              2.2310e-01, -1.7625e-02],\n",
       "            [-9.3424e-02,  1.5406e-01,  5.8276e-02,  ..., -2.0751e-01,\n",
       "             -1.9436e-03,  1.6982e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-8.2378e-01, -4.8960e-01, -3.8344e-01,  ...,  5.2290e-01,\n",
       "              5.0372e-02,  5.2475e-01],\n",
       "            [-6.0074e-01,  4.8117e-01,  7.7945e-02,  ...,  1.0213e+00,\n",
       "              1.6829e-01,  2.2406e-01],\n",
       "            [-1.4064e+00,  8.5268e-01,  1.5904e+00,  ...,  9.1617e-01,\n",
       "              1.6421e-01,  3.3579e-01],\n",
       "            ...,\n",
       "            [-1.6556e-01, -1.3409e+00,  4.7458e-01,  ...,  9.2571e-01,\n",
       "              7.1910e-02,  7.9859e-02],\n",
       "            [ 1.1588e-01, -3.2367e-01, -6.9891e-01,  ..., -3.6446e-01,\n",
       "              7.6197e-02,  1.9810e-01],\n",
       "            [-3.0882e-01, -2.9376e-02, -3.3734e-01,  ...,  1.2380e-01,\n",
       "             -6.3336e-02,  1.8064e-01]],\n",
       "  \n",
       "           [[-1.2429e-01,  6.0300e-02,  5.2340e-02,  ...,  5.5954e-01,\n",
       "             -9.7001e-02,  5.1732e-02],\n",
       "            [ 2.7051e-01, -2.4512e-02, -6.3767e-01,  ..., -4.9245e-01,\n",
       "              4.6650e-01,  4.0388e-01],\n",
       "            [-1.0137e+00, -7.1698e-01, -2.0812e+00,  ..., -5.7009e-01,\n",
       "              4.9118e-01,  4.1616e-01],\n",
       "            ...,\n",
       "            [-2.4101e-01, -2.1609e-01,  4.4005e-01,  ...,  2.8598e-01,\n",
       "             -4.5900e-01,  1.2424e-01],\n",
       "            [-3.0166e-01,  1.1980e-01, -3.0490e-01,  ..., -2.3896e-01,\n",
       "              1.1229e-01,  1.1606e-01],\n",
       "            [ 1.3570e-02, -3.7005e-02,  1.7478e-01,  ...,  3.7245e-02,\n",
       "              5.9096e-01,  2.0899e-01]],\n",
       "  \n",
       "           [[-1.5865e-01, -5.9506e-01, -3.2611e-01,  ..., -5.4269e-01,\n",
       "             -1.9581e-01, -4.5673e-01],\n",
       "            [-3.3528e-01, -2.5954e-01, -1.5724e+00,  ..., -7.7990e-01,\n",
       "             -9.8112e-01, -4.5997e-02],\n",
       "            [ 6.8551e-01, -5.2950e-01, -1.1161e+00,  ...,  9.1702e-02,\n",
       "              8.1607e-01, -1.3858e-01],\n",
       "            ...,\n",
       "            [-8.8949e-01,  9.8869e-02,  7.0970e-01,  ..., -7.5759e-01,\n",
       "             -5.8078e-01, -3.4089e-01],\n",
       "            [-4.8718e-01,  2.8895e-01,  6.6661e-01,  ...,  2.2361e-01,\n",
       "              3.9646e-01, -3.7129e-01],\n",
       "            [ 1.0620e-01, -3.2722e-01,  4.3511e-01,  ..., -2.7836e-01,\n",
       "              1.1440e-03,  1.4684e-01]]]], grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[-8.7700e-02,  6.2080e-01, -4.7680e-02,  8.1278e-02],\n",
       "            [ 2.4714e-01,  9.3296e-01,  8.1130e-01,  7.8492e-01],\n",
       "            [ 3.1993e-01,  1.9883e-01, -1.3832e+00, -4.5455e-01],\n",
       "            [-3.7950e-02, -4.1508e-01,  3.1636e-01, -8.6570e-01]],\n",
       "  \n",
       "           [[ 3.9606e-01, -3.9337e-02,  5.3481e-02, -3.3381e-01],\n",
       "            [ 2.8022e-01, -1.1041e+00,  2.1258e-01, -1.2819e-01],\n",
       "            [ 3.7308e-01, -1.1720e+00,  1.0751e+00, -7.3931e-02],\n",
       "            [ 1.4194e-01, -3.5884e-01, -1.9987e-01, -6.0002e-01]],\n",
       "  \n",
       "           [[-3.4969e-02,  8.8431e-02, -1.6024e-01,  2.9153e-01],\n",
       "            [-3.6616e-02, -1.0020e+00, -9.2743e-01, -6.0238e-02],\n",
       "            [-2.5952e-01, -4.6467e-01, -3.1030e-01,  2.8853e-01],\n",
       "            [-3.9395e-01,  4.4877e-01, -1.1888e-01, -4.1435e-01]],\n",
       "  \n",
       "           [[ 2.5607e-01, -1.0354e-01, -8.0108e-01,  4.4254e-01],\n",
       "            [-1.2613e-01, -1.0556e+00, -1.0619e+00,  1.4512e-01],\n",
       "            [-1.3914e-03,  5.3007e-02, -7.8722e-02,  5.4463e-01],\n",
       "            [-1.0970e-02,  3.7567e-01,  4.7500e-02, -6.3210e-02]],\n",
       "  \n",
       "           [[ 2.7012e-01, -1.5685e-01, -1.0701e-01,  4.4017e-01],\n",
       "            [-1.8349e-03,  3.3935e-02,  9.2471e-01,  9.0563e-01],\n",
       "            [ 3.2698e-02,  4.4468e-01, -4.3349e-01, -3.8540e-01],\n",
       "            [-2.8616e-01,  2.9696e-01,  4.4605e-01,  1.7106e-01]],\n",
       "  \n",
       "           [[ 1.7370e-01, -7.3253e-01,  2.4469e-01, -4.8492e-01],\n",
       "            [-2.4120e-01, -5.8509e-01, -1.2793e-01, -4.8554e-01],\n",
       "            [ 4.2935e-01, -1.0154e+00,  6.3697e-01,  4.0300e-02],\n",
       "            [ 3.3186e-01,  9.6607e-01,  4.1808e-01, -5.6742e-01]],\n",
       "  \n",
       "           [[-6.7347e-02,  1.1299e+00,  3.3286e-01,  4.1881e-01],\n",
       "            [-2.1591e-01,  5.9704e-01,  1.0294e+00,  7.8073e-01],\n",
       "            [-1.3329e-01,  1.0719e+00, -3.1650e-03, -3.5909e-01],\n",
       "            [-5.2637e-01,  3.0845e-01, -6.6409e-01,  9.9069e-02]],\n",
       "  \n",
       "           [[-4.5912e-01, -4.8850e-01, -9.4390e-02,  5.3770e-01],\n",
       "            [-4.6008e-01, -5.8629e-01, -4.6285e-01, -5.3343e-01],\n",
       "            [-4.5545e-02, -7.7402e-01, -1.3853e+00, -1.4474e-01],\n",
       "            [ 2.1664e-01,  3.4379e-01, -3.3979e-01, -3.4244e-01]],\n",
       "  \n",
       "           [[ 1.2600e-01, -9.8714e-01,  4.9430e-01,  3.3045e-01],\n",
       "            [ 2.2221e-01,  7.0838e-01,  2.5616e-01, -1.8936e-01],\n",
       "            [ 3.0790e-01,  5.3659e-02,  5.5196e-01,  4.3183e-01],\n",
       "            [-9.7996e-02,  4.5822e-01, -2.0134e-02,  3.3998e-01]],\n",
       "  \n",
       "           [[-3.9282e-01,  4.7119e-02,  4.4387e-01,  7.5987e-01],\n",
       "            [ 5.1071e-02, -2.0964e-01, -3.3766e-01, -1.3063e-01],\n",
       "            [ 4.6740e-01,  9.8702e-02, -4.2955e-01, -9.1761e-03],\n",
       "            [ 5.0221e-01, -1.7413e-01, -2.8444e-01, -3.6474e-01]],\n",
       "  \n",
       "           [[-5.7480e-01, -5.0316e-01,  8.2121e-01, -1.6379e-01],\n",
       "            [ 3.4054e-01,  1.5207e-01,  1.3518e+00, -2.1978e-01],\n",
       "            [ 2.3320e-01, -1.5122e-01, -8.3457e-01,  2.3121e-01],\n",
       "            [ 4.2831e-01, -5.9425e-01, -7.1990e-02,  2.4155e-01]],\n",
       "  \n",
       "           [[ 3.8475e-02,  6.7249e-01,  5.4186e-02,  2.2848e-01],\n",
       "            [-1.2026e-01,  3.9984e-01,  3.1082e-02,  2.5155e-01],\n",
       "            [ 4.7630e-01,  4.1041e-01,  1.7816e+00,  5.4428e-01],\n",
       "            [ 2.6793e-01,  6.9625e-01, -6.5755e-01,  1.8588e-01]],\n",
       "  \n",
       "           [[ 8.4799e-03, -5.8794e-01,  8.6908e-01,  3.4582e-01],\n",
       "            [-3.8264e-01, -1.9083e+00,  9.9086e-01,  4.2640e-01],\n",
       "            [-2.3302e-01, -3.1257e-01,  6.1820e-01, -1.2509e-01],\n",
       "            [-1.8126e-01, -1.3743e-01, -1.0993e-01, -2.1999e-01]],\n",
       "  \n",
       "           [[ 4.0214e-01,  4.0389e-01,  3.0305e-01, -3.0232e-01],\n",
       "            [ 2.6413e-01, -5.2887e-01,  3.1429e-01,  8.9104e-01],\n",
       "            [ 6.3190e-01,  1.2219e+00, -2.6898e-01,  2.6175e-01],\n",
       "            [ 2.0822e-01,  1.0101e-01,  1.5554e+00, -2.4789e-01]],\n",
       "  \n",
       "           [[-2.7104e-01, -1.9808e-01, -2.3745e-01,  1.4184e-01],\n",
       "            [ 1.1514e+00, -8.5184e-01,  3.5954e-01, -3.8447e-01],\n",
       "            [ 6.7188e-01, -6.6814e-01, -2.2969e-01,  1.2873e+00],\n",
       "            [ 6.1845e-01, -2.7236e-01, -9.5586e-01,  3.8325e-03]],\n",
       "  \n",
       "           [[-1.5450e-01, -1.3587e+00,  6.0042e-01,  3.4957e-01],\n",
       "            [ 2.2250e-01,  1.1962e-02,  1.7995e-01, -5.1738e-01],\n",
       "            [ 4.3333e-02, -3.8912e-01,  4.1532e-01, -1.6771e-01],\n",
       "            [ 8.1776e-01,  6.1602e-02, -5.0755e-01,  3.8401e-01]],\n",
       "  \n",
       "           [[ 8.8832e-02,  2.5711e-01,  4.4231e-03,  2.2012e-01],\n",
       "            [-3.4346e-01, -7.1417e-01, -9.0533e-01,  3.2927e-01],\n",
       "            [ 5.4617e-01,  8.3678e-02,  1.9778e-01, -3.2990e-01],\n",
       "            [-2.2137e-01, -8.1688e-02,  9.7117e-01, -2.9112e-01]],\n",
       "  \n",
       "           [[ 1.7145e-01, -4.6401e-01,  1.3209e-01, -4.2914e-01],\n",
       "            [-1.9359e-01, -5.6633e-02, -9.4505e-02,  2.8275e-01],\n",
       "            [ 5.4857e-01,  4.2493e-02,  4.3344e-02, -4.7491e-01],\n",
       "            [-5.6118e-02, -4.5214e-01,  8.9818e-01,  7.4964e-02]],\n",
       "  \n",
       "           [[-4.9091e-02, -2.9098e-02, -5.4479e-01,  5.7964e-01],\n",
       "            [-3.9435e-01,  9.1417e-01, -4.2403e-01,  1.1669e-01],\n",
       "            [-2.5720e-01,  2.1751e-01,  3.3511e-01, -9.5342e-01],\n",
       "            [ 1.8436e-02,  4.3843e-01, -6.9037e-01, -1.4955e-01]],\n",
       "  \n",
       "           [[-8.4332e-02, -3.2117e-01,  3.5270e-01, -7.7333e-01],\n",
       "            [-2.6343e-01,  1.6754e+00, -7.4296e-01, -1.2376e+00],\n",
       "            [ 3.4370e-01,  5.6515e-01,  5.0956e-01, -6.3746e-01],\n",
       "            [-1.8163e-01,  6.6916e-01,  1.3146e-01,  4.1256e-01]],\n",
       "  \n",
       "           [[ 1.0366e-01, -7.9853e-01, -9.4754e-01, -6.6147e-01],\n",
       "            [ 6.0912e-03, -1.1103e-01,  5.8068e-01, -2.8843e-01],\n",
       "            [ 4.7528e-01, -2.3309e-02,  5.4884e-01, -8.7085e-02],\n",
       "            [-4.6558e-01,  2.1254e-01, -1.7618e-01,  7.0363e-02]],\n",
       "  \n",
       "           [[ 3.8186e-01,  8.5295e-02, -4.9972e-01,  1.7302e-01],\n",
       "            [ 4.6656e-01, -4.2431e-01, -4.5895e-01, -7.1939e-02],\n",
       "            [-1.6594e-01, -8.4267e-01,  1.8793e-01,  1.1571e-01],\n",
       "            [ 7.3071e-02, -1.1906e-01,  6.1585e-02,  7.2207e-02]],\n",
       "  \n",
       "           [[ 1.8818e-01,  2.6137e-01, -6.6301e-01, -1.0175e-01],\n",
       "            [ 6.5498e-02, -1.7921e-02,  7.5348e-01,  6.4870e-01],\n",
       "            [ 5.3282e-01, -3.1269e-01, -1.6760e+00,  3.1035e-01],\n",
       "            [-3.3689e-01,  3.4097e-01,  5.9631e-01, -5.2082e-01]],\n",
       "  \n",
       "           [[-3.1028e-01,  9.0435e-02,  4.9671e-01, -7.3387e-02],\n",
       "            [-2.1004e-02,  2.0195e-01,  4.3110e-01,  3.2802e-01],\n",
       "            [-1.6389e-01,  5.8977e-02, -1.6071e-01,  1.6635e-01],\n",
       "            [-1.7089e-01, -4.8970e-01,  1.2149e-01,  3.2053e-02]],\n",
       "  \n",
       "           [[ 3.3670e-01, -8.5721e-01,  2.4006e-01, -4.0243e-01],\n",
       "            [ 2.8998e-01, -5.4544e-02,  8.5153e-01, -2.0689e-01],\n",
       "            [-2.5508e-01, -4.5120e-01, -2.5729e-01, -3.3595e-01],\n",
       "            [ 1.0987e-01, -6.0977e-01,  1.2029e-01, -5.8182e-01]],\n",
       "  \n",
       "           [[ 1.4267e-01, -2.5226e-01,  6.6531e-01, -5.3118e-01],\n",
       "            [ 3.0239e-01, -1.3103e-02,  9.7046e-01, -1.7615e-01],\n",
       "            [-5.3085e-01, -6.9357e-01, -2.6178e-01, -3.8632e-01],\n",
       "            [-1.7910e-01,  9.2066e-02,  3.4145e-01,  8.7394e-02]],\n",
       "  \n",
       "           [[ 1.8136e-01,  5.4222e-01,  7.1349e-01, -2.1647e-01],\n",
       "            [ 5.3797e-01,  6.6226e-01,  1.1436e+00, -9.8646e-02],\n",
       "            [ 6.9468e-02,  8.8272e-02, -1.1643e-01, -6.9993e-01],\n",
       "            [-2.8893e-01, -1.6289e-01,  2.3085e-02,  2.2130e-01]],\n",
       "  \n",
       "           [[-1.2850e-01,  2.4963e-01,  2.3423e-01,  1.0452e-01],\n",
       "            [-6.7065e-01, -2.1190e-01, -6.9902e-02,  1.3310e+00],\n",
       "            [-1.0971e+00, -2.1587e-01, -1.0024e+00,  3.4060e-01],\n",
       "            [-8.8440e-01,  3.8863e-01,  1.5417e-01, -2.1205e-01]],\n",
       "  \n",
       "           [[ 1.4784e-01,  2.5919e-01, -6.5441e-02, -5.6905e-01],\n",
       "            [-6.7182e-03,  2.6913e-01, -1.5957e-01,  3.9957e-01],\n",
       "            [-6.7610e-01, -2.6109e-01,  3.3921e-01,  5.6595e-02],\n",
       "            [ 5.4213e-02, -2.1641e-01,  5.4298e-01,  1.6228e-03]],\n",
       "  \n",
       "           [[-5.2811e-01, -8.2834e-01,  2.8083e-01, -1.1406e-01],\n",
       "            [ 2.6720e-01, -5.6754e-01, -2.1534e-01, -1.2560e-02],\n",
       "            [-2.1361e-01, -1.2618e-02, -2.8825e-01,  1.5293e-02],\n",
       "            [-4.6726e-02, -6.1444e-02, -5.4743e-01, -5.3738e-01]],\n",
       "  \n",
       "           [[-3.2947e-01,  1.0721e-01,  1.5832e-01, -4.6263e-01],\n",
       "            [ 5.1591e-02,  1.3277e-01,  1.2593e+00, -7.2878e-02],\n",
       "            [-3.9795e-01, -6.3880e-01, -3.5857e-01,  4.8526e-01],\n",
       "            [ 7.1880e-02, -8.5161e-02, -4.9044e-01, -4.1589e-01]],\n",
       "  \n",
       "           [[-1.5673e-01, -1.5307e-01, -2.7470e-01,  8.2902e-01],\n",
       "            [ 2.0646e-01,  2.2625e-02, -5.2783e-01, -2.9598e-01],\n",
       "            [-3.0882e-01,  1.4272e-01, -6.5018e-01, -1.3467e-01],\n",
       "            [-9.1966e-02,  1.9212e-01,  7.4059e-01, -6.3072e-02]],\n",
       "  \n",
       "           [[ 1.2208e-01, -6.4975e-01, -2.4115e-01,  4.8348e-01],\n",
       "            [-1.9220e-01,  3.7004e-01, -1.2677e+00,  2.6231e-02],\n",
       "            [-8.5257e-02, -4.6957e-01,  2.8993e-01,  9.0876e-01],\n",
       "            [-5.4009e-02,  4.0351e-01,  1.4685e-01,  2.1277e-01]],\n",
       "  \n",
       "           [[-1.1227e-01, -1.9945e-01,  1.4857e-01,  9.3598e-01],\n",
       "            [-6.7499e-01,  3.6187e-01,  3.0244e-01,  3.9110e-01],\n",
       "            [-6.4274e-01,  4.2727e-01, -1.0159e+00, -5.1041e-01],\n",
       "            [-2.6440e-01, -4.1020e-01,  2.0879e-01,  5.9230e-02]],\n",
       "  \n",
       "           [[-3.6897e-01,  9.0127e-01, -2.4709e-01,  6.0206e-01],\n",
       "            [-9.2172e-01,  3.3066e-01,  9.5098e-01,  5.9111e-01],\n",
       "            [-2.0193e-01, -5.6264e-03, -6.6609e-01, -5.1969e-01],\n",
       "            [-5.3942e-01, -8.4141e-01, -4.8055e-01, -4.7114e-01]],\n",
       "  \n",
       "           [[ 2.8693e-01,  1.9131e-01, -1.1088e+00, -8.5288e-01],\n",
       "            [ 1.3933e-02, -7.1120e-01,  5.3337e-01,  3.2931e-01],\n",
       "            [-1.0446e-01,  6.8786e-01, -2.7064e-01, -1.0146e-01],\n",
       "            [ 1.8644e-01,  5.6069e-01,  1.8526e-01,  1.8178e-01]]]],\n",
       "         grad_fn=<ThnnConv2DBackward>),\n",
       "  tensor([[[[ 0.0097, -0.6410],\n",
       "            [-0.0552,  0.5548]],\n",
       "  \n",
       "           [[-0.4599, -0.1745],\n",
       "            [ 0.2992, -0.6803]],\n",
       "  \n",
       "           [[ 0.1586, -0.3879],\n",
       "            [-0.0999, -0.9290]],\n",
       "  \n",
       "           [[-0.3431,  0.0921],\n",
       "            [ 0.3413, -0.1715]],\n",
       "  \n",
       "           [[-0.0629, -0.0262],\n",
       "            [ 0.0827,  0.3270]],\n",
       "  \n",
       "           [[-0.3411, -0.2573],\n",
       "            [-0.1030, -0.0155]],\n",
       "  \n",
       "           [[-0.6057,  0.4082],\n",
       "            [ 0.4575,  0.2277]],\n",
       "  \n",
       "           [[-0.2992, -0.5057],\n",
       "            [-0.1717,  0.2869]],\n",
       "  \n",
       "           [[-0.0587,  0.0484],\n",
       "            [-0.0998,  0.5430]],\n",
       "  \n",
       "           [[ 0.1564, -0.2519],\n",
       "            [-0.2662, -0.0196]],\n",
       "  \n",
       "           [[-0.1564,  0.1133],\n",
       "            [ 0.0935,  0.1616]],\n",
       "  \n",
       "           [[-0.0604,  0.3725],\n",
       "            [-0.0567,  0.3416]],\n",
       "  \n",
       "           [[-0.1384,  0.0820],\n",
       "            [-0.2631, -0.0740]],\n",
       "  \n",
       "           [[ 0.0402,  0.2222],\n",
       "            [ 0.0770,  0.2624]],\n",
       "  \n",
       "           [[ 0.4378, -0.1688],\n",
       "            [-0.0417, -0.1132]],\n",
       "  \n",
       "           [[ 0.6716, -0.1364],\n",
       "            [ 0.0149,  0.7093]],\n",
       "  \n",
       "           [[-0.3620, -0.1345],\n",
       "            [ 0.3365, -0.3680]],\n",
       "  \n",
       "           [[-0.2522, -0.3296],\n",
       "            [ 0.0046,  0.3222]],\n",
       "  \n",
       "           [[ 0.2910,  0.0836],\n",
       "            [-0.4195,  0.5305]],\n",
       "  \n",
       "           [[-0.2197, -0.2363],\n",
       "            [ 0.1261,  0.2042]],\n",
       "  \n",
       "           [[-0.3225, -0.1120],\n",
       "            [-0.3451,  0.3867]],\n",
       "  \n",
       "           [[ 0.1962,  0.3874],\n",
       "            [ 0.6276, -0.1971]],\n",
       "  \n",
       "           [[-0.0702,  0.5463],\n",
       "            [-0.3975,  0.0755]],\n",
       "  \n",
       "           [[ 0.5237, -0.5287],\n",
       "            [-0.5780, -0.1509]],\n",
       "  \n",
       "           [[ 0.2729, -0.3540],\n",
       "            [ 0.0530, -0.2783]],\n",
       "  \n",
       "           [[ 0.1904, -0.4324],\n",
       "            [-0.2973, -0.2302]],\n",
       "  \n",
       "           [[ 0.2482, -0.5587],\n",
       "            [-0.3305, -0.1206]],\n",
       "  \n",
       "           [[-0.5537,  0.3219],\n",
       "            [-0.1771, -0.0016]],\n",
       "  \n",
       "           [[ 0.5082, -0.1448],\n",
       "            [ 0.0337,  0.1965]],\n",
       "  \n",
       "           [[ 0.0559, -0.4407],\n",
       "            [-0.3118,  0.0356]],\n",
       "  \n",
       "           [[-0.6999, -0.1836],\n",
       "            [-0.2554, -0.3136]],\n",
       "  \n",
       "           [[ 0.2705,  0.0169],\n",
       "            [ 0.0823, -0.3618]],\n",
       "  \n",
       "           [[ 0.2256, -0.1859],\n",
       "            [-0.4058,  0.3268]],\n",
       "  \n",
       "           [[ 0.1889,  0.0842],\n",
       "            [-0.8840,  1.0007]],\n",
       "  \n",
       "           [[-0.0726,  0.2166],\n",
       "            [-0.2655,  0.0103]],\n",
       "  \n",
       "           [[ 0.5862, -0.3434],\n",
       "            [-0.1937,  0.4822]]]], grad_fn=<ThnnConv2DBackward>)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in model.model(img):\n",
    "    for j in i:\n",
    "        print(j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in model.model(img):\n",
    "    print(len(i))\n",
    "    for j in i:\n",
    "        print(j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=256)\n",
    "\n",
    "x = img\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    print(name, end=\"\\t\")\n",
    "    if name == \"avgpool\":\n",
    "        x = x.permute(0, 2, 1)\n",
    "    if name == \"head\":\n",
    "        x = x.squeeze(-1)\n",
    "    print(x.shape, end=\"\\t\")\n",
    "    x = eval(f\"model.{name}(x)\")\n",
    "    print(x.shape)\n",
    "\n",
    "assert (x == model(img)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if \"projects\" not in os.getcwd():\n",
    "  !git clone --depth 1 https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch\n",
    "  os.chdir('Yet-Another-EfficientDet-Pytorch')\n",
    "  sys.path.append('.')\n",
    "else:\n",
    "  !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=0)\n",
    "summary(model, input_data=img, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c78c3tjbE6d"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USEhrYiYHLSx"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1621175027575,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "lutFP6pmayNN"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pandas.read_csv(TRAIN_CSV_PATH, names=range(4), skiprows=1)\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data[3] = self.df_data[3].fillna(\"\")\n",
    "        self.df_data[2] += self.df_data[3]\n",
    "        self.df_data = self.df_data.drop(3, axis=1)\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 1: \"label\", 2: \"caption\"}, axis=1).dropna()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        label = self.df_data.iloc[idx, 1]\n",
    "        caption = self.df_data.iloc[idx, 2]\n",
    "\n",
    "        sample = {\"caption\": caption, \"label\": label, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPE_x_qeGden"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1621175027576,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "oMHDsQa5pbvJ"
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pandas.read_csv(TEST_CSV_PATH, names=range(3), skiprows=1)\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data[2] = self.df_data[2].fillna(\"\")\n",
    "        self.df_data[1] += self.df_data[2]\n",
    "        self.df_data = self.df_data.drop(2, axis=1)\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 1: \"caption\"}, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        caption = self.df_data.iloc[idx, 1]\n",
    "\n",
    "        sample = {\"caption\": caption, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R5B99nfXD3g"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3scsujfGr18"
   },
   "source": [
    "### Dataset pre-transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1621175030901,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "zbL0EmlyeNTm"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(dataset):\n",
    "    if \"one_hot_encoded_labels\" not in dataset.tags:\n",
    "        dataset.df_data[\"label\"] = dataset.df_data[\"label\"].apply(\n",
    "            lambda l: torch.nn.functional.one_hot(\n",
    "                torch.tensor([int(i) - 1 if int(i) < 12 else int(i) - 2 for i in l.split(\" \")]), 18\n",
    "            )\n",
    "            .sum(axis=0)\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        dataset.tags.add(\"one_hot_encoded_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpTCDn6_Vrco"
   },
   "source": [
    "## Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCCA7khCHOnG"
   },
   "source": [
    "### Caption embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ecrmj0tdk78V",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ztan/miniconda3/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6fb796fd814e4596453a7631943949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/990 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43af8efa67c4965bcc5f45d93ca637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e98276b78b84f3297448e8f049a8c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff3d0095d37449ca25fcefba516f51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c228b4d85be446a99640ccbbced388e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/246k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sshleifer/tiny-distilbert-base-cased were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "# lang_model = AutoModel.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-distilbert-base-cased\")\n",
    "lang_model = AutoModel.from_pretrained(\"sshleifer/tiny-distilbert-base-cased\")\n",
    "\n",
    "for name, param in lang_model.named_parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 2, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 2)\n",
       "    (token_type_embeddings): Embedding(2, 2)\n",
       "    (LayerNorm): LayerNorm((2,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (key): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (value): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (LayerNorm): LayerNorm((2,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=2, out_features=4, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4, out_features=2, bias=True)\n",
       "          (LayerNorm): LayerNorm((2,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (key): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (value): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=2, out_features=2, bias=True)\n",
       "            (LayerNorm): LayerNorm((2,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=2, out_features=4, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4, out_features=2, bias=True)\n",
       "          (LayerNorm): LayerNorm((2,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vial1naJBNC7"
   },
   "source": [
    "### Pretrained model surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6hUrTmdVBj0Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19}\n",
    "# eff_net = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "\n",
    "swin = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=256)\n",
    "\n",
    "\n",
    "class SWIN(nn.Module):\n",
    "    def __init__(self, word_dim=256):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=word_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        vec = None\n",
    "        out = None\n",
    "        for name, child in self.model.named_children():\n",
    "            if name == \"avgpool\":\n",
    "                vec = self.model.head(x)\n",
    "                out = self.model.avgpool(x.permute(0, 2, 1)).squeeze(-1)\n",
    "                return out, vec\n",
    "            x = eval(f\"self.model.{name}(x)\")\n",
    "\n",
    "\n",
    "# features = Flattened(eff_net)\n",
    "features = SWIN(256)\n",
    "for name, param in features.named_parameters():\n",
    "    param.requires_grad_(False)\n",
    "#     print(name) # layers.3.blocks.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([1, 49, 256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "img = torch.randn(1,3,224,224)\n",
    "# model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=256)\n",
    "\n",
    "x = img\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "#     print(x.shape)\n",
    "\n",
    "# features = Flattened(model)\n",
    "\n",
    "# assert (x == model(img)).all()\n",
    "features(img)[0].shape, features(img)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v44fbNqCOtv9"
   },
   "source": [
    "### Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUtVf9QdOxnN"
   },
   "outputs": [],
   "source": [
    "NUM_ClASSES = 18\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "img_dim = 768\n",
    "word_dim = list(lang_model.pooler.parameters())[-1].shape[0]\n",
    "\n",
    "\n",
    "class SASGA(nn.Module):\n",
    "    def __init__(self, word_dim=word_dim):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = norm = nn.LayerNorm(word_dim, eps=1e-05, elementwise_affine=True)\n",
    "        self.layer_norm_2 = norm = nn.LayerNorm(word_dim, eps=1e-05, elementwise_affine=True)\n",
    "\n",
    "        self.guided_attention_layer = nn.TransformerDecoderLayer(\n",
    "            word_dim, nhead=word_dim // 64, dim_feedforward=word_dim, dropout=0.1, activation=\"relu\"\n",
    "        )\n",
    "        self.guided_attention = nn.TransformerDecoder(\n",
    "            self.guided_attention_layer, num_layers=1, norm=self.layer_norm_1\n",
    "        )\n",
    "        self.self_attention_layer = nn.TransformerDecoderLayer(\n",
    "            word_dim, nhead=word_dim // 64, dim_feedforward=word_dim, dropout=0.1, activation=\"relu\"\n",
    "        )\n",
    "        self.self_attention = nn.TransformerDecoder(\n",
    "            self.self_attention_layer, num_layers=1, norm=self.layer_norm_2\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y, y_mask):\n",
    "        # guide x by y\n",
    "        x = self.self_attention_layer(x, x)\n",
    "        x = self.guided_attention(x, y, memory_key_padding_mask=y_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Combined_Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, visual_features, lang_model, word_dim=word_dim, img_dim=img_dim, num_decoder_layers=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.visual_features = visual_features\n",
    "        self.lang_model = lang_model\n",
    "        self.img_dim = img_dim\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "        self.bottle_neck = nn.Conv2d(in_channels=img_dim, out_channels=word_dim, kernel_size=1)\n",
    "        self.layer_norm = norm = nn.LayerNorm(word_dim, eps=1e-05, elementwise_affine=True)\n",
    "\n",
    "        # SA-GA ##############################\n",
    "        self.SASGA = SASGA(word_dim)\n",
    "        # SA-GA ##############################\n",
    "\n",
    "        self.output = nn.Linear(word_dim, NUM_ClASSES)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.proj_all = nn.Linear(img_dim + word_dim * 2, word_dim)\n",
    "\n",
    "    def forward(self, images, inputs):  # requires tokenized captions\n",
    "        lang_outs = self.lang_model(**inputs)\n",
    "        last_hidden_state = lang_outs[\"last_hidden_state\"].permute(1, 0, 2)  # seq_len,\n",
    "        pooler_out = lang_outs[\"pooler_output\"]\n",
    "        mask = inputs.attention_mask == 0\n",
    "\n",
    "        images, image_query = self.visual_features(images)\n",
    "#         image_query = intermediate_outputs[\"_conv_head\"]\n",
    "#         image_query = self.bottle_neck(image_query)\n",
    "\n",
    "        # no param attention:\n",
    "        #         image_query = image_query.view(-1, self.word_dim, 49)\n",
    "        #         last_hidden_state = lang_outs[\"last_hidden_state\"] # batch, seq_len, word_dim\n",
    "\n",
    "        #         attention_weights = torch.bmm(last_hidden_state, image_query) # batch, seq_len, 49\n",
    "        #         att_out = torch.bmm(attention_weights.permute(0,2,1), last_hidden_state).mean(1)\n",
    "        #         att_out = self.layer_norm(att_out+image_query.mean(-1))\n",
    "\n",
    "        #####################################################\n",
    "        # self attention\n",
    "        # image_query = image_query.view(-1, self.word_dim, 49).permute(2, 0, 1)\n",
    "        # att_out = self.self_attention_layer(image_query, image_query)\n",
    "\n",
    "        # #         # guided attention\n",
    "        # att_out = self.guided_attention(att_out, last_hidden_state, memory_key_padding_mask=mask)\n",
    "        # att_out = att_out.permute(1, 2, 0)  # 49, batch, word_dim -> batch, word_dim, 49\n",
    "        # att_out = att_out.mean(-1)  # -> batch, word_dim\n",
    "        #####################################################\n",
    "        image_query = image_query.permute(1, 0, 2)\n",
    "\n",
    "        att_out = self.SASGA(image_query, last_hidden_state, mask).mean(0).squeeze(0)\n",
    "#         print(att_out.shape)\n",
    "        out = torch.cat([images, att_out, pooler_out], dim=-1)\n",
    "        out = self.proj_all(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, _ in model.module_list.named_parameters():\n",
    "#     print(name, _.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srZFInwa78_j"
   },
   "source": [
    "## Transforms and training support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1621175053522,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "MvOT-Plt78Zp"
   },
   "outputs": [],
   "source": [
    "class FieldTransform(object):\n",
    "    def __init__(self, field, transform):\n",
    "        self.field = field\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample[self.field] = self.transform(sample[self.field])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZHM_qQiQniU"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1621175053523,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "oPqavf7wWfiB"
   },
   "outputs": [],
   "source": [
    "def train_collate_fn(X):\n",
    "    # convert [{key: val, ...}, ...]\n",
    "    # to [key: [val, ...],  ...}\n",
    "    X = {k: [v[k] for v in X] for k in X[0]}\n",
    "    X[\"label\"] = torch.stack(X[\"label\"])\n",
    "    X[\"image\"] = torch.stack(X[\"image\"])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        FieldTransform(\"image\", torchvision.transforms.Resize((224, 224))),\n",
    "        FieldTransform(\"image\", torchvision.transforms.ToTensor()),\n",
    "        FieldTransform(\"image\", normalize),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2421,
     "status": "ok",
     "timestamp": 1621175110701,
     "user": {
      "displayName": "Isomorphism__",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCPGQv-x0fGHL4DUlrvt2EuQaICcM6lEXnzGpA=s64",
      "userId": "06837884690357263397"
     },
     "user_tz": -600
    },
    "id": "o_5acvhCWjSn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = TrainDataset(transform=transforms)\n",
    "# vocabularise_caption(train_data, vocab)\n",
    "one_hot_encode_labels(train_data)\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# train_data.__len__() == 30000\n",
    "trainds, valds = random_split(train_data, [27000, 3000])\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    trainds, batch_size=32, shuffle=True, collate_fn=train_collate_fn, num_workers=24\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valds, batch_size=8, shuffle=False, collate_fn=train_collate_fn, num_workers=24)\n",
    "\n",
    "# model = Combined().to(DEVICE)\n",
    "\n",
    "\n",
    "# model = Combined_Model(features, lang_model, num_decoder_layers=1).to(device)\n",
    "model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=18)\n",
    "model.to(device)\n",
    "\n",
    "from sam import SAM\n",
    "\n",
    "# base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\n",
    "# optimizer = SAM(model.parameters(), base_optimizer, lr=5e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    threshold=0.0001,\n",
    "    threshold_mode=\"rel\",\n",
    "    cooldown=5,\n",
    "    min_lr=0,\n",
    "    eps=1e-08,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for name, param in model.module_list.named_parameters():\n",
    "#     print(name)\n",
    "# raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"test_v8.pt\") #check model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuhLuCQxQnR1",
    "outputId": "bde8bc1d-7820-460e-8d89-e78e8af216d4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.bias\n",
      "layers.0.blocks.0.norm1.weight\n",
      "layers.0.blocks.0.norm1.bias\n",
      "layers.0.blocks.0.attn.relative_position_bias_table\n",
      "layers.0.blocks.0.attn.qkv.weight\n",
      "layers.0.blocks.0.attn.qkv.bias\n",
      "layers.0.blocks.0.attn.proj.weight\n",
      "layers.0.blocks.0.attn.proj.bias\n",
      "layers.0.blocks.0.norm2.weight\n",
      "layers.0.blocks.0.norm2.bias\n",
      "layers.0.blocks.0.mlp.fc1.weight\n",
      "layers.0.blocks.0.mlp.fc1.bias\n",
      "layers.0.blocks.0.mlp.fc2.weight\n",
      "layers.0.blocks.0.mlp.fc2.bias\n",
      "layers.0.blocks.1.norm1.weight\n",
      "layers.0.blocks.1.norm1.bias\n",
      "layers.0.blocks.1.attn.relative_position_bias_table\n",
      "layers.0.blocks.1.attn.qkv.weight\n",
      "layers.0.blocks.1.attn.qkv.bias\n",
      "layers.0.blocks.1.attn.proj.weight\n",
      "layers.0.blocks.1.attn.proj.bias\n",
      "layers.0.blocks.1.norm2.weight\n",
      "layers.0.blocks.1.norm2.bias\n",
      "layers.0.blocks.1.mlp.fc1.weight\n",
      "layers.0.blocks.1.mlp.fc1.bias\n",
      "layers.0.blocks.1.mlp.fc2.weight\n",
      "layers.0.blocks.1.mlp.fc2.bias\n",
      "layers.0.downsample.reduction.weight\n",
      "layers.0.downsample.norm.weight\n",
      "layers.0.downsample.norm.bias\n",
      "layers.1.blocks.0.norm1.weight\n",
      "layers.1.blocks.0.norm1.bias\n",
      "layers.1.blocks.0.attn.relative_position_bias_table\n",
      "layers.1.blocks.0.attn.qkv.weight\n",
      "layers.1.blocks.0.attn.qkv.bias\n",
      "layers.1.blocks.0.attn.proj.weight\n",
      "layers.1.blocks.0.attn.proj.bias\n",
      "layers.1.blocks.0.norm2.weight\n",
      "layers.1.blocks.0.norm2.bias\n",
      "layers.1.blocks.0.mlp.fc1.weight\n",
      "layers.1.blocks.0.mlp.fc1.bias\n",
      "layers.1.blocks.0.mlp.fc2.weight\n",
      "layers.1.blocks.0.mlp.fc2.bias\n",
      "layers.1.blocks.1.norm1.weight\n",
      "layers.1.blocks.1.norm1.bias\n",
      "layers.1.blocks.1.attn.relative_position_bias_table\n",
      "layers.1.blocks.1.attn.qkv.weight\n",
      "layers.1.blocks.1.attn.qkv.bias\n",
      "layers.1.blocks.1.attn.proj.weight\n",
      "layers.1.blocks.1.attn.proj.bias\n",
      "layers.1.blocks.1.norm2.weight\n",
      "layers.1.blocks.1.norm2.bias\n",
      "layers.1.blocks.1.mlp.fc1.weight\n",
      "layers.1.blocks.1.mlp.fc1.bias\n",
      "layers.1.blocks.1.mlp.fc2.weight\n",
      "layers.1.blocks.1.mlp.fc2.bias\n",
      "layers.1.downsample.reduction.weight\n",
      "layers.1.downsample.norm.weight\n",
      "layers.1.downsample.norm.bias\n",
      "layers.2.blocks.0.norm1.weight\n",
      "layers.2.blocks.0.norm1.bias\n",
      "layers.2.blocks.0.attn.relative_position_bias_table\n",
      "layers.2.blocks.0.attn.qkv.weight\n",
      "layers.2.blocks.0.attn.qkv.bias\n",
      "layers.2.blocks.0.attn.proj.weight\n",
      "layers.2.blocks.0.attn.proj.bias\n",
      "layers.2.blocks.0.norm2.weight\n",
      "layers.2.blocks.0.norm2.bias\n",
      "layers.2.blocks.0.mlp.fc1.weight\n",
      "layers.2.blocks.0.mlp.fc1.bias\n",
      "layers.2.blocks.0.mlp.fc2.weight\n",
      "layers.2.blocks.0.mlp.fc2.bias\n",
      "layers.2.blocks.1.norm1.weight\n",
      "layers.2.blocks.1.norm1.bias\n",
      "layers.2.blocks.1.attn.relative_position_bias_table\n",
      "layers.2.blocks.1.attn.qkv.weight\n",
      "layers.2.blocks.1.attn.qkv.bias\n",
      "layers.2.blocks.1.attn.proj.weight\n",
      "layers.2.blocks.1.attn.proj.bias\n",
      "layers.2.blocks.1.norm2.weight\n",
      "layers.2.blocks.1.norm2.bias\n",
      "layers.2.blocks.1.mlp.fc1.weight\n",
      "layers.2.blocks.1.mlp.fc1.bias\n",
      "layers.2.blocks.1.mlp.fc2.weight\n",
      "layers.2.blocks.1.mlp.fc2.bias\n",
      "layers.2.blocks.2.norm1.weight\n",
      "layers.2.blocks.2.norm1.bias\n",
      "layers.2.blocks.2.attn.relative_position_bias_table\n",
      "layers.2.blocks.2.attn.qkv.weight\n",
      "layers.2.blocks.2.attn.qkv.bias\n",
      "layers.2.blocks.2.attn.proj.weight\n",
      "layers.2.blocks.2.attn.proj.bias\n",
      "layers.2.blocks.2.norm2.weight\n",
      "layers.2.blocks.2.norm2.bias\n",
      "layers.2.blocks.2.mlp.fc1.weight\n",
      "layers.2.blocks.2.mlp.fc1.bias\n",
      "layers.2.blocks.2.mlp.fc2.weight\n",
      "layers.2.blocks.2.mlp.fc2.bias\n",
      "layers.2.blocks.3.norm1.weight\n",
      "layers.2.blocks.3.norm1.bias\n",
      "layers.2.blocks.3.attn.relative_position_bias_table\n",
      "layers.2.blocks.3.attn.qkv.weight\n",
      "layers.2.blocks.3.attn.qkv.bias\n",
      "layers.2.blocks.3.attn.proj.weight\n",
      "layers.2.blocks.3.attn.proj.bias\n",
      "layers.2.blocks.3.norm2.weight\n",
      "layers.2.blocks.3.norm2.bias\n",
      "layers.2.blocks.3.mlp.fc1.weight\n",
      "layers.2.blocks.3.mlp.fc1.bias\n",
      "layers.2.blocks.3.mlp.fc2.weight\n",
      "layers.2.blocks.3.mlp.fc2.bias\n",
      "layers.2.blocks.4.norm1.weight\n",
      "layers.2.blocks.4.norm1.bias\n",
      "layers.2.blocks.4.attn.relative_position_bias_table\n",
      "layers.2.blocks.4.attn.qkv.weight\n",
      "layers.2.blocks.4.attn.qkv.bias\n",
      "layers.2.blocks.4.attn.proj.weight\n",
      "layers.2.blocks.4.attn.proj.bias\n",
      "layers.2.blocks.4.norm2.weight\n",
      "layers.2.blocks.4.norm2.bias\n",
      "layers.2.blocks.4.mlp.fc1.weight\n",
      "layers.2.blocks.4.mlp.fc1.bias\n",
      "layers.2.blocks.4.mlp.fc2.weight\n",
      "layers.2.blocks.4.mlp.fc2.bias\n",
      "layers.2.blocks.5.norm1.weight\n",
      "layers.2.blocks.5.norm1.bias\n",
      "layers.2.blocks.5.attn.relative_position_bias_table\n",
      "layers.2.blocks.5.attn.qkv.weight\n",
      "layers.2.blocks.5.attn.qkv.bias\n",
      "layers.2.blocks.5.attn.proj.weight\n",
      "layers.2.blocks.5.attn.proj.bias\n",
      "layers.2.blocks.5.norm2.weight\n",
      "layers.2.blocks.5.norm2.bias\n",
      "layers.2.blocks.5.mlp.fc1.weight\n",
      "layers.2.blocks.5.mlp.fc1.bias\n",
      "layers.2.blocks.5.mlp.fc2.weight\n",
      "layers.2.blocks.5.mlp.fc2.bias\n",
      "layers.2.downsample.reduction.weight\n",
      "layers.2.downsample.norm.weight\n",
      "layers.2.downsample.norm.bias\n",
      "layers.3.blocks.0.norm1.weight\n",
      "layers.3.blocks.0.norm1.bias\n",
      "layers.3.blocks.0.attn.relative_position_bias_table\n",
      "layers.3.blocks.0.attn.qkv.weight\n",
      "layers.3.blocks.0.attn.qkv.bias\n",
      "layers.3.blocks.0.attn.proj.weight\n",
      "layers.3.blocks.0.attn.proj.bias\n",
      "layers.3.blocks.0.norm2.weight\n",
      "layers.3.blocks.0.norm2.bias\n",
      "layers.3.blocks.0.mlp.fc1.weight\n",
      "layers.3.blocks.0.mlp.fc1.bias\n",
      "layers.3.blocks.0.mlp.fc2.weight\n",
      "layers.3.blocks.0.mlp.fc2.bias\n",
      "layers.3.blocks.1.norm1.weight\n",
      "layers.3.blocks.1.norm1.bias\n",
      "layers.3.blocks.1.attn.relative_position_bias_table\n",
      "layers.3.blocks.1.attn.qkv.weight\n",
      "layers.3.blocks.1.attn.qkv.bias\n",
      "layers.3.blocks.1.attn.proj.weight\n",
      "layers.3.blocks.1.attn.proj.bias\n",
      "layers.3.blocks.1.norm2.weight\n",
      "layers.3.blocks.1.norm2.bias\n",
      "layers.3.blocks.1.mlp.fc1.weight\n",
      "layers.3.blocks.1.mlp.fc1.bias\n",
      "layers.3.blocks.1.mlp.fc2.weight\n",
      "layers.3.blocks.1.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:14<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, train/val loss:(0.2225636, 0.2015422),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n",
      "saving best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:14<00:00,  6.29it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, train/val loss:(0.2035046, 0.2025641),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:11<00:00,  6.43it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2, train/val loss:(0.2029522, 0.2005512),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:14<00:00,  6.28it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3, train/val loss:(0.2030299, 0.2006484),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:04<00:00,  6.80it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4, train/val loss:(nan, 0.2007779),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [01:59<00:00,  7.05it/s]\n",
      "  0%|          | 0/844 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5, train/val loss:(nan, 0.2007779),sample/mean f1:(0.647846, 0.6024285), lr:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 844/844 [02:00<00:00,  6.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-144e19f1838b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#         with torch.cuda.amp.autocast():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#         predictions = model(images, inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_pos_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B L C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B C 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# merge windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mattn_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_windows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mshifted_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B H' W' C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# reverse cyclic shift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mwindow_reverse\u001b[0;34m(windows, window_size, H, W)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "val_mean_f1 = []\n",
    "val_sample_f1 = []\n",
    "model_in_memory = {}\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(50):\n",
    "    train_loss = []\n",
    "    train_outs = []\n",
    "    train_lables = []\n",
    "    #     model.train()\n",
    "\n",
    "    #     fine tune CNN for 5 epoches\n",
    "#     if epoch == 0:\n",
    "#         finetune_param = False\n",
    "#         for name, param in model.visual_features.named_parameters():\n",
    "#             if \"layers.3.blocks.1\" in name:\n",
    "#                 finetune_param = True\n",
    "#             if finetune_param:\n",
    "#                 print(name)\n",
    "#                 param.requires_grad_(True)\n",
    "\n",
    "    if epoch == 0:\n",
    "#         finetune_param = False\n",
    "#         for name, param in model.lang_model.named_parameters():\n",
    "#             if \"pooler\" in name:\n",
    "#                 finetune_param = True\n",
    "#             if finetune_param:\n",
    "#                 print(name)\n",
    "#                 param.requires_grad_(True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dl)):\n",
    "        optimizer.zero_grad()\n",
    "#         captions = batch[\"caption\"]\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "#         inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "        #         raise ValueError\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "        # first second step\n",
    "#             predictions = model(images, inputs)\n",
    "            predictions = model(images)\n",
    "            loss = criterion(predictions, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        #         with torch.cuda.amp.autocast():\n",
    "        # second step\n",
    "#         predictions = model(images, inputs)\n",
    "#         loss = criterion(predictions, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.second_step(zero_grad=True)\n",
    "        #\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    val_loss = []\n",
    "    val_outs = []\n",
    "    val_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(val_dl):\n",
    "\n",
    "#         captions = batch[\"caption\"]\n",
    "        images = batch[\"image\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "#         inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "        #         predictions = model(images, captions)\n",
    "        #         with torch.cuda.amp.autocast():\n",
    "#         predictions = model(images, inputs)\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        val_outs.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "    val_labels = np.vstack(val_labels)\n",
    "    val_outs = np.vstack(val_outs)\n",
    "    mean_f1 = f1_score(y_true=val_labels, y_pred=1 * (val_outs > 0), average=\"micro\")  # mean f1\n",
    "    sample_f1 = f1_score(y_true=val_labels, y_pred=1 * (val_outs > 0), average=\"samples\")  # mean f1\n",
    "\n",
    "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\n",
    "        f\"Epoch:{epoch}, train/val loss:{round(np.mean(train_loss),7),round(np.mean(val_loss),7)},sample/mean f1:{round(sample_f1, 7), round(mean_f1, 7)}, lr:{cur_lr}\"\n",
    "    )\n",
    "    scheduler.step(np.mean(val_loss))\n",
    "\n",
    "    if len(val_sample_f1) == 0 or sample_f1 > max(val_sample_f1):\n",
    "        print(\"saving best\")\n",
    "        try:\n",
    "            del model_in_memory[\"best\"]\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        model.to(\"cpu\")\n",
    "        model_in_memory[\"best\"] = deepcopy(model.state_dict())\n",
    "        model.to(device)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model_in_memory[\"best\"], \"test_v8.pt\")\n",
    "\n",
    "    val_mean_f1.append(mean_f1)\n",
    "    val_sample_f1.append(sample_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError\n",
    "# (finetune last 3 layer (15-17)) + (SAM, lr 0.001)\n",
    "# b0 + bert tiny + no attention = 85\n",
    "# b0 + bert tiny + SAGA * 1 = 85 ...\n",
    "# b0 + bert tiny + bmm (attention but no weights) at last conv filters = fails..?! (lr too high )\n",
    "# b0 (13-) + bert tiny + bmm (attention but no weights) = 85\n",
    "# b0 + bert tiny (both no finetune) + SAGA * 1 (5e-5 lr)= 86.40\n",
    "# b2 + bert mini (both no finetune) + SAGA * 1 (5e-5 lr)= 86.3\n",
    "# b2 + bert mini (finetune from 21/3 attention) + SAGA * 1 (5e-5 lr)= 87.4..\n",
    "# swin = 89.2\n",
    "# swin by itself: doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = lang_model(**inputs)\n",
    "last_hidden_state = outs.last_hidden_state\n",
    "pooler_output = outs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_feats, all_outs = features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outs[\"_conv_head\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.shape, last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bottle_neck = nn.Conv2d(in_channels=1408, out_channels=256, kernel_size=1).to(device)\n",
    "norm = nn.LayerNorm(256, eps=1e-05, elementwise_affine=True)\n",
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    256, 8, dim_feedforward=1024, dropout=0.1, activation=\"relu\"\n",
    ")\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=1, norm=norm).to(device)\n",
    "\n",
    "comp = bottle_neck(all_outs[\"_conv_head\"]).view(-1, 256, 49).permute(2, 0, 1)\n",
    "decoder(comp, last_hidden_state, memory_key_padding_mask=(inputs.attention_mask == 0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "images = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "eff_net = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "\n",
    "for name, param in eff_net.named_parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "class Flattened(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.module_list = nn.ModuleList()\n",
    "        self.module_names = []\n",
    "        for name, child in pretrained_model.named_children():\n",
    "            if name == \"_fc\":\n",
    "                continue  # get rid of last layer\n",
    "            if isinstance(child, nn.ModuleList):\n",
    "                for idx, i in enumerate(child):\n",
    "                    self.module_list.append(i)\n",
    "                    self.module_names.append(f\"{name}_{str(idx)}\")\n",
    "            else:\n",
    "                self.module_list.append(child)\n",
    "                self.module_names.append(name)\n",
    "        self._swish_idx = self.module_names.index(\"_swish\")\n",
    "\n",
    "    def forward(self, img_batch):\n",
    "        all_outs = {}\n",
    "        x = img_batch\n",
    "        for idx, module in enumerate(self.module_list):\n",
    "            if self.module_names[idx] == \"_swish\":\n",
    "                continue\n",
    "            elif self.module_names[idx] == \"_dropout\":\n",
    "                shape = x.shape\n",
    "                x = module(x.view(-1, x.shape[1]))\n",
    "            else:\n",
    "                x = module(x)\n",
    "\n",
    "            all_outs[self.module_names[idx]] = x\n",
    "            if \"bn\" in self.module_names[idx]:\n",
    "                x = self.module_list[self._swish_idx](x)\n",
    "        return x, all_outs\n",
    "\n",
    "\n",
    "eff_net.eval()\n",
    "eff_net\n",
    "f = Flattened(eff_net)\n",
    "x, all_outs = f(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(eff_net, input, output):\n",
    "        activation[name] = output\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "for name, child in eff_net.named_children():\n",
    "    if isinstance(child, nn.ModuleList):\n",
    "        for idx, i in enumerate(child):\n",
    "            i.register_forward_hook(get_activation(f\"{name}_{str(idx)}\"))\n",
    "    else:\n",
    "        child.register_forward_hook(get_activation(name))\n",
    "\n",
    "\n",
    "output = eff_net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in activation.keys():\n",
    "    if key == \"_swish\":\n",
    "        continue\n",
    "    print(key)\n",
    "    print((all_outs[key] == activation[key]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eff_net.to(\"cuda\")\n",
    "# f.to(\"cuda\")\n",
    "\n",
    "# %timeit -n10 -r5 eff_net(images.cuda())\n",
    "# %timeit -n10 -r5 f(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 1\n",
    "summary(f, input_size=(batch_size, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in all_outs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "208 * 3 + 352 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"test.pt\"\n",
    "# model = LUKE_CRF(tag_to_ix, lang_model=lang_model, hidden_dim=1024)\n",
    "model = Combined_Model(features, lang_model)\n",
    "model.load_state_dict(torch.load(\"test.pt\"))\n",
    "model.to(device)\n",
    "# model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# model = torch.load(model_name)\n",
    "# # use the same validation set/training set\n",
    "# val_outs = []\n",
    "# val_labels = []\n",
    "# model.eval()\n",
    "# for i, batch in enumerate(val_dl):\n",
    "\n",
    "#     captions = batch[\"caption\"]\n",
    "#     images = batch[\"image\"].to(DEVICE)\n",
    "#     labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "#     inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "#     input_ids = inputs[\"input_ids\"].to(device)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#     # predictions = model(images, captions)\n",
    "#     predictions = model(images, input_ids, attention_mask)\n",
    "\n",
    "#     val_outs.append(predictions.detach().cpu().numpy())\n",
    "#     val_labels.append(labels.detach().cpu().numpy())\n",
    "# val_labels = np.vstack(val_labels)\n",
    "# val_outs = np.vstack(val_outs)\n",
    "\n",
    "# best_thresholds = np.zeros(18)\n",
    "# for i in range(18):\n",
    "#     fpr, tpr, thresholds = roc_curve(val_labels[:, i], (val_outs)[:, i])\n",
    "#     gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "#     ix = np.argmax(gmeans)\n",
    "#     best_thresholds[i] = thresholds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cheat\n",
    "class CheatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.tags = set()\n",
    "        self.df_data = pd.read_csv(\"test_cheat.csv\", names=range(3), skiprows=1)[[0, 2, 1]].dropna()\n",
    "        self.df_data[0] = IMG_PATH + \"/\" + self.df_data[0]\n",
    "        self.df_data = self.df_data.rename({0: \"image\", 2: \"label\", 1: \"caption\"}, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = PIL.Image.open(self.df_data.iloc[idx, 0])\n",
    "        label = self.df_data.iloc[idx, 1]\n",
    "        caption = self.df_data.iloc[idx, 2]\n",
    "\n",
    "        sample = {\"caption\": caption, \"label\": label, \"image\": image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "cheat_data = CheatDataset(transform=transforms)\n",
    "# vocabularise_caption(train_data, vocab)\n",
    "one_hot_encode_labels(cheat_data)\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# train_data.__len__() == 30000\n",
    "cheat_dl = DataLoader(\n",
    "    cheat_data, batch_size=32, shuffle=False, collate_fn=train_collate_fn, num_workers=24\n",
    ")\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# use the same validation set/training set\n",
    "val_outs = []\n",
    "val_labels = []\n",
    "model.eval()\n",
    "for i, batch in enumerate(cheat_dl):\n",
    "\n",
    "    captions = batch[\"caption\"]\n",
    "    images = batch[\"image\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "    # predictions = model(images, captions)\n",
    "    predictions = model(images, inputs)\n",
    "\n",
    "    val_outs.append(predictions.detach().cpu().numpy())\n",
    "    val_labels.append(labels.detach().cpu().numpy())\n",
    "val_labels = np.vstack(val_labels)\n",
    "val_outs = np.vstack(val_outs)\n",
    "\n",
    "\n",
    "best_thresholds = np.zeros(18)\n",
    "for i in range(18):\n",
    "    fpr, tpr, thresholds = precision, recall, thresholds = precision_recall_curve(\n",
    "        val_labels[:, i], (val_outs)[:, i]\n",
    "    )\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    best_thresholds[i] = thresholds[ix]\n",
    "\n",
    "print(best_thresholds)\n",
    "np.save(\"best_thresholds\", best_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopper for run all\n",
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_collate_fn(X):\n",
    "    # convert [{key: val, ...}, ...]\n",
    "    # to [key: [val, ...],  ...}\n",
    "    X = {k: [v[k] for v in X] for k in X[0]}\n",
    "    X[\"image\"] = torch.stack(X[\"image\"])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "test_data = TestDataset(transform=transforms)\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=16, shuffle=False, collate_fn=test_collate_fn, num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "# save predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "for i, batch in enumerate(test_dl):\n",
    "    captions = batch[\"caption\"]\n",
    "    inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "    predictions = model(images, inputs)\n",
    "    test_preds.append(predictions.detach().cpu().numpy())\n",
    "\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "\n",
    "def out_logits_to_preds(logits, best_thresholds):\n",
    "    labels = []\n",
    "    logits = (logits - np.zeros_like(best_thresholds)) > 0\n",
    "    lables_available = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19])\n",
    "    for i in range(logits.shape[0]):\n",
    "        labels.append(list(lables_available[logits[i]]))\n",
    "    return labels\n",
    "\n",
    "\n",
    "# lables_available[(test_preds[:10] > 0)]\n",
    "# {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19}\n",
    "\n",
    "test_labels = out_logits_to_preds(test_preds, best_thresholds)\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH, names=range(3), skiprows=1)\n",
    "test_labels_str = [\" \".join([str(i) for i in labels]) for labels in test_labels]\n",
    "df_test[\"Labels\"] = test_labels_str\n",
    "df_test.rename({0: \"ImageID\"}, axis=1, inplace=True)\n",
    "\n",
    "df_test[[\"ImageID\", \"Labels\"]].to_csv(\"test_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "COMP5329_Assignment_2_v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "105bdd467ce44751b3020eddd3d4c791": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37766d0f97754f21a9b78ec7c2c9b266": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b5faf89e145490b8df3c5a0974a30e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37766d0f97754f21a9b78ec7c2c9b266",
      "max": 21388428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0e1d4db2eb74c24887d9b9d63e36a2a",
      "value": 21388428
     }
    },
    "6c592ed619724f1997199bf45b9727e7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775c2d8bc3824da89165a6baf24327c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b5faf89e145490b8df3c5a0974a30e0",
       "IPY_MODEL_fbc0ba310f384411ad658bc8ceee5607"
      ],
      "layout": "IPY_MODEL_6c592ed619724f1997199bf45b9727e7"
     }
    },
    "9929ab1374f74db7addc12be1eaa6a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0e1d4db2eb74c24887d9b9d63e36a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fbc0ba310f384411ad658bc8ceee5607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_105bdd467ce44751b3020eddd3d4c791",
      "placeholder": "",
      "style": "IPY_MODEL_9929ab1374f74db7addc12be1eaa6a29",
      "value": " 20.4M/20.4M [00:00&lt;00:00, 70.0MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
